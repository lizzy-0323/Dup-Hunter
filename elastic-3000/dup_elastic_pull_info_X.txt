[{"A_title": "Die cwd die", "A_clean_title": ["die", "cwd", "die"], "B_title": "Only check existence for absolute paths in env.resolveConfig()", "B_clean_title": ["onli", "check", "exist", "absolut", "path", "env", "resolveconfig", "resolv", "config"], "A_body": "See #10877 for reference.\n\nCurrently some code looks in CWD and places like $user.home for configuration files in a confusing way, in some cases even overriding other paths (so if you have a stray elasticsearch.yml there, good luck). \n\nTests hide the problem because they add too many permissions. We fixed that here and then all tests were failing, and made hacky changes to get tests passing again.\n\nSo some of the changes here can be cleaned up / solved differently. For example we could look for the custom hunspell data directory in Environment, grant access to it, and deprecate it instead of throwing an error. But, with what is here all tests pass so I think we know enough to discuss so we can move forward.\n", "A_clean_body": ["see", "10877", "refer", "current", "some", "code", "look", "cwd", "place", "like", "user", "home", "configur", "file", "confus", "way", "some", "case", "even", "overrid", "other", "path", "so", "you", "have", "stray", "elasticsearch", "yml", "there", "good", "luck", "test", "hide", "problem", "becaus", "they", "add", "too", "mani", "permiss", "we", "fix", "that", "here", "then", "all", "test", "were", "fail", "made", "hacki", "chang", "get", "test", "pass", "again", "so", "some", "chang", "here", "clean", "up", "solv", "differ", "exampl", "we", "could", "look", "custom", "hunspel", "data", "directori", "environ", "grant", "access", "it", "deprec", "it", "instead", "throw", "error", "but", "what", "here", "all", "test", "pass", "so", "think", "we", "know", "enough", "discuss", "so", "we", "move", "forward"], "B_body": "When resolving a path using `env.resolveConfig(String)`, it first checks for the path existence in the current working directory. This can fails when the security manager is enabled and elasticsearch not started from the ES_HOME directory like with init.d/systemd scripts or also with ./path/to/elasticsearch/bin/elasticsearch: Files.exist() resolves against the working directory and fails with a SecurityException.\n\nI'm not sure we always need to set \"user.dir\" so I add a check for absolute path. This couldn't hurt much since the following `f1.toUri().toURL()` expects an absolute path.\n\nWhat do you think?\n", "B_clean_body": ["when", "resolv", "path", "env", "resolveconfig", "resolv", "config", "string", "it", "first", "check", "path", "exist", "current", "work", "directori", "thi", "fail", "when", "secur", "manag", "enabl", "elasticsearch", "not", "start", "es", "home", "directori", "like", "init", "systemd", "script", "or", "also", "path", "elasticsearch", "bin", "elasticsearch", "file", "exist", "resolv", "against", "work", "directori", "fail", "securityexcept", "secur", "except", "'m", "not", "sure", "we", "alway", "need", "set", "user", "dir", "so", "add", "check", "absolut", "path", "thi", "could", "n't", "hurt", "much", "sinc", "follow", "f1", "touri", "uri", "tourl", "url", "expect", "absolut", "path", "what", "you", "think"], "title_sim": [0.0], "body_sim": [0.31505140265000553], "file_list_sim": 0.02040816326530612, "overlap_files_len": 1, "code_sim": [0.0, 0.0], "location_sim": [0.01763668430335097, 0.45454545454545453], "pattern": 0, "time": 3}, {"A_title": "[DOCS] Update fielddata.asciidoc", "A_clean_title": ["doc", "updat", "fielddata", "asciidoc"], "B_title": "[DOC] Fix the other default values for filter cache size and field data ...", "B_clean_title": ["doc", "fix", "other", "default", "valu", "filter", "cach", "size", "field", "data"], "A_body": "Spelling correction and corrected breaker limit default value.\n", "A_clean_body": ["spell", "correct", "correct", "breaker", "limit", "default", "valu"], "B_body": "...circuit\n\nRelates to #5990\n", "B_clean_body": ["circuit", "relat", "5990"], "title_sim": [0.07893790583140517], "body_sim": [0.2436522677523855], "file_list_sim": 1.0, "overlap_files_len": 1, "code_sim": [1.0, 1.0], "location_sim": [1.0, 1.0], "pattern": 0, "time": 0}, {"A_title": "Remove Triple Negative", "A_clean_title": ["remov", "tripl", "neg"], "B_title": "Double negation removing", "B_clean_title": ["doubl", "negat", "remov"], "A_body": "Double negatives are confusing, but a triple negative?!\n\n> no non null\n\nIt takes five minutes to understand this little sentence.  Cleaned that up a bit.\n", "A_clean_body": ["doubl", "neg", "are", "confus", "but", "tripl", "neg", "no", "non", "null", "it", "take", "five", "minut", "understand", "thi", "littl", "sentenc", "clean", "that", "up", "bit"], "B_body": "It seems like the sentence would be more understandable without the double negation.\n", "B_clean_body": ["it", "seem", "like", "sentenc", "would", "more", "understand", "without", "doubl", "negat"], "title_sim": [0.46652191970811074], "body_sim": [0.2104031874082818], "file_list_sim": 1.0, "overlap_files_len": 1, "code_sim": [0.0, 0.0], "location_sim": [0.6666666666666666, 0.6666666666666666], "pattern": 0, "time": 3}, {"A_title": "Factor out slow logs into Search and IndexingOperationListeners", "A_clean_title": ["factor", "out", "slow", "log", "into", "search", "indexingoperationlisten", "index", "oper", "listen"], "B_title": "Expose query/fetch timing to IndexEventListener", "B_clean_title": ["expos", "queri", "fetch", "time", "indexeventlisten", "index", "event", "listen"], "A_body": "This commit introduces SearchOperationListeneres which allow to hook\ninto search operation lifecycle and execute operations like slow-logs\nand statistic collection in a transparent way. SearchOperationListenrs\ncan be registered on the IndexModule just like IndexingOperationListeners.\nThe main consumers (slow log) have already been moved out of IndexService\ninto IndexModule which reduces the dependency on IndexService as well as\nIndexShard and makes slowlogging transparent.\n", "A_clean_body": ["thi", "commit", "introduc", "searchoperationlistener", "search", "oper", "listener", "which", "allow", "hook", "into", "search", "oper", "lifecycl", "execut", "oper", "like", "slow", "log", "statist", "collect", "transpar", "way", "searchoperationlistenr", "search", "oper", "listenr", "regist", "indexmodul", "index", "modul", "just", "like", "indexingoperationlisten", "index", "oper", "listen", "main", "consum", "slow", "log", "have", "alreadi", "been", "move", "out", "indexservic", "index", "servic", "into", "indexmodul", "index", "modul", "which", "reduc", "depend", "indexservic", "index", "servic", "as", "well", "as", "indexshard", "index", "shard", "make", "slowlog", "transpar"], "B_body": "Elasticsearch collects a limited number of stats that can be collected in a very\nperformant manner. Other stats, such as latency percentiles, are perhaps out of\nscope of the main elasticsearch project. This allows plugins to add their own\nstat collection routines to the existing shard stats collection.\n\nCloses #15412\n", "B_clean_body": ["elasticsearch", "collect", "limit", "number", "stat", "that", "collect", "veri", "perform", "manner", "other", "stat", "such", "as", "latenc", "percentil", "are", "perhap", "out", "scope", "main", "elasticsearch", "project", "thi", "allow", "plugin", "add", "their", "own", "stat", "collect", "routin", "exist", "shard", "stat", "collect", "close", "15412"], "title_sim": [0.2266655965361339], "body_sim": [0.08951836365068748], "file_list_sim": 0.125, "overlap_files_len": 2, "code_sim": [0.40046463910648633, 0.3116959913414166], "location_sim": [0.06407487401007919, 0.5085714285714286], "pattern": 0, "time": 105}, {"A_title": "Make bin/elasticsearch wait for pidfile", "A_clean_title": ["make", "bin", "elasticsearch", "wait", "pidfil"], "B_title": "background start-up: use PID file to signal success", "B_clean_title": ["background", "start", "up", "use", "pid", "file", "signal", "success"], "A_body": "If bin/elasticsearch is run with the option to daemonize and the option to\nwrite a pidfile then it will wait for 30 seconds for Elasticsearch to write\nthe pidfile. If it fails to write the pidfile before the timeout then it\nwill warn the user to check the logs and further warn them that if nothing\nshows up in the logs that they should attempt to run Elasticsearch in the\nforeground.\n\nCloses #13392\n", "A_clean_body": ["bin", "elasticsearch", "run", "option", "daemon", "option", "write", "pidfil", "then", "it", "will", "wait", "30", "second", "elasticsearch", "write", "pidfil", "it", "fail", "write", "pidfil", "befor", "timeout", "then", "it", "will", "warn", "user", "check", "log", "further", "warn", "them", "that", "noth", "show", "up", "log", "that", "they", "attempt", "run", "elasticsearch", "foreground", "close", "13392"], "B_body": "This change introduces the use of elasticsearch's PID file to signal a\nsuccessful start-up. The start script `bin/elasticsearch` will wait a\nconfigurable amount of time (default: 120s) for the PID file to appear. After\nthis timed out the script will print an error and return an error code.\n\nNote that if no PID file was specified on the command line the script will now\nuse `mktemp` to create a \"private\" PID file (only if ES is to be started in the\nbackground, though). The command line help was updated correspondingly.\n\nAlso, the PID file generation code has been moved from the beginning of the\nbootstrap `main()` method to the end.\n\nFixes #8652.\n\n**NOTE**\nPlease review my changes to `src/main/java/org/elasticsearch/bootstrap/Bootstrap.java` very closely; I'm not too familiar with ES code and am not sure if I broke something.\n\n**NOTE 2**\nThis change introduces a (theoretical) new error condition: If elasticsearch requires longer than the timeout to bootstrap, a \"false positive\" start-up error will be reported by the script even though elasticsearch will be up and running eventually. This can be worked around by setting an appropriately long timeout via the new `-w` command line option.\n", "B_clean_body": ["thi", "chang", "introduc", "use", "elasticsearch", "'s", "pid", "file", "signal", "success", "start", "up", "start", "script", "bin", "elasticsearch", "will", "wait", "configur", "amount", "time", "default", "120", "pid", "file", "appear", "after", "thi", "time", "out", "script", "will", "print", "error", "return", "error", "code", "note", "that", "no", "pid", "file", "wa", "specifi", "command", "line", "script", "will", "now", "use", "mktemp", "creat", "privat", "pid", "file", "onli", "es", "start", "background", "though", "command", "line", "help", "wa", "updat", "correspondingli", "also", "pid", "file", "gener", "code", "ha", "been", "move", "begin", "bootstrap", "main", "method", "end", "fix", "8652", "**note**", "pleas", "review", "my", "chang", "java", "src", "main", "java", "org", "elasticsearch", "bootstrap", "bootstrap", "veri", "close", "'m", "not", "too", "familiar", "es", "code", "am", "not", "sure", "broke", "someth", "**note", "2**", "thi", "chang", "introduc", "theoret", "new", "error", "condit", "elasticsearch", "requir", "longer", "than", "timeout", "bootstrap", "fals", "posit", "start", "up", "error", "will", "report", "by", "script", "even", "though", "elasticsearch", "will", "up", "run", "eventu", "thi", "work", "around", "by", "set", "appropri", "long", "timeout", "via", "new", "command", "line", "option"], "title_sim": [0.01219320800827163], "body_sim": [0.2273105187631873], "file_list_sim": 0.0, "overlap_files_len": 0, "code_sim": [0.0, 0.0], "location_sim": [0.0, 0.0], "pattern": -1, "time": 289}, {"A_title": "use spaces liberally in integration tests and fix space handling", "A_clean_title": ["use", "space", "liber", "integr", "test", "fix", "space", "handl"], "B_title": "Startup: Disable globbing in shell script", "B_clean_title": ["startup", "disabl", "glob", "shell", "script"], "A_body": "By using pathnames with spaces in tests we can kickout all the bugs.\nI applied the fix for #12709 but we needed more fixes actually.\n\nTODO: windows\n", "A_clean_body": ["by", "pathnam", "space", "test", "we", "kickout", "all", "bug", "appli", "fix", "12709", "but", "we", "need", "more", "fix", "actual", "todo", "window"], "B_body": "In order to not accidentally expand wilcard arguments\nlike --http.cors.allow-origin '*' on startup, globbing\nneeds to be disabled before Elasticsearch is started.\n\nCloses #12689\n", "B_clean_body": ["order", "not", "accident", "expand", "wilcard", "argument", "like", "origin", "http", "cor", "allow", "startup", "glob", "need", "disabl", "befor", "elasticsearch", "start", "close", "12689"], "title_sim": [0.007376041955108713], "body_sim": [0.04059627491715432], "file_list_sim": 0.25, "overlap_files_len": 1, "code_sim": [0.0, 0.0], "location_sim": [0.15254237288135594, 1.0], "pattern": -1, "time": 0}, {"A_title": "Search Templates: Adds API endpoint to render search templates as a response", "A_clean_title": ["search", "templat", "add", "api", "endpoint", "render", "search", "templat", "as", "respons"], "B_title": "Indexed script fixes", "B_clean_title": ["index", "script", "fix"], "A_body": "Adds a '_render/template' endpoint which will render a search template in the response with a provided `params` object.\n\nCloses #6821\n", "A_clean_body": ["add", "render", "templat", "endpoint", "which", "will", "render", "search", "templat", "respons", "provid", "param", "object", "close", "6821"], "B_body": " On Disk Scripts : Make on disk scripts re-load timing dynamically configurable.\nThis change adds a new resource watcher level of CUSTOM and allows the addition of custom levels of timing.\nThis is used by the ScriptService to register the FileWatcher.\n\nSearch Templates: Add render endpoint for rendering templates\nThis commit adds a render endpoint to allow rendering of templates.\n\n```\nPOST /_render/template\n'{\n  \"template\":\n  {\n    \"query\":\n      {\n        \"{{foo}}\": {}\n      },\n    \"size\": \"{{my_{{}}size}}\"\n   },\n   \"params\": { \"foo\": \"match_all\", \"my_size\": 10}\n}'\n```\n\nWill render\n\n```\n{\n  \"template\" :\n  {\n    \"query\" : {\n      \"match_all\" : {}\n    },\n    \"size\" : 10\n  }\n}\n```\n\nCloses #6821\n", "B_clean_body": ["disk", "script", "make", "disk", "script", "re", "load", "time", "dynam", "configur", "thi", "chang", "add", "new", "resourc", "watcher", "level", "custom", "allow", "addit", "custom", "level", "time", "thi", "use", "by", "scriptservic", "script", "servic", "regist", "filewatch", "file", "watcher", "search", "templat", "add", "render", "endpoint", "render", "templat", "thi", "commit", "add", "render", "endpoint", "allow", "render", "templat", "post", "render", "templat", "templat", "queri", "foo", "size", "my", "size", "param", "foo", "match", "all", "my", "size", "10", "will", "render", "templat", "queri", "match", "all", "size", "10", "close", "6821"], "title_sim": [-0.013243525402233568], "body_sim": [0.5250956696864111], "file_list_sim": 0.034482758620689655, "overlap_files_len": 1, "code_sim": [0.34843116573047744, 0.7673501800101775], "location_sim": [0.05819477434679335, 1.0], "pattern": 1, "time": 252}, {"A_title": "Provides a cat api endpoint for templates.", "A_clean_title": ["provid", "cat", "api", "endpoint", "templat"], "B_title": "[DOCS] Add \"version\" to template and pipeline docs", "B_clean_title": ["doc", "add", "version", "templat", "pipelin", "doc"], "A_body": "Adds a cat api endpoint: `/_cat/templates` and its more specific version, `/_cat/templates/{name}`.\n\nIt looks something like:\n\n```\n$ curl \"localhost:9200/_cat/templates?v\"\nname                  template     order version\nsushi_california_roll *avocado*    1     1\npizza_hawaiian        *pineapples* 1\npizza_pepperoni       *pepperoni*  1\n```\n\nThe specified version (only allows \\* globs) looks like:\n\n```\n$ curl \"localhost:9200/_cat/templates/pizza*\"\nname            template     order version\npizza_hawaiian  *pineapples* 1\npizza_pepperoni *pepperoni*  1\n```\n\nPartially specified columns:\n\n```\n$ curl \"localhost:9200/_cat/templates/pizza*?v=true&h=name,template\"\nname            template\npizza_hawaiian  *pineapples*\npizza_pepperoni *pepperoni*\n```\n\nThe help text:\n\n```\n$ curl \"localhost:9200/_cat/templates/pizza*?help\"\nname     | n | template name\ntemplate | t | template pattern string\norder    | o | template application order number\nversion  | v | version\n```\n\nCloses #20467 \n", "A_clean_body": ["add", "cat", "api", "endpoint", "cat", "templat", "it", "more", "specif", "version", "cat", "templat", "name", "it", "look", "someth", "like", "curl", "localhost:9200", "cat", "templat", "name", "templat", "order", "version", "sushi", "california", "roll", "*avocado*", "pizza", "hawaiian", "*pineapples*", "pizza", "pepperoni", "*pepperoni*", "specifi", "version", "onli", "allow", "glob", "look", "like", "curl", "localhost:9200", "cat", "templat", "pizza*", "name", "templat", "order", "version", "pizza", "hawaiian", "*pineapples*", "pizza", "pepperoni", "*pepperoni*", "partial", "specifi", "column", "curl", "localhost:9200", "cat", "templat", "pizza*", "v=true", "h=name", "templat", "name", "templat", "pizza", "hawaiian", "*pineapples*", "pizza", "pepperoni", "*pepperoni*", "help", "text", "curl", "localhost:9200", "cat", "templat", "pizza*", "help", "name", "templat", "name", "templat", "templat", "pattern", "string", "order", "templat", "applic", "order", "number", "version", "version", "close", "20467"], "B_body": "This adds details about the `\"version\"` field to both the template and pipeline pages.\n", "B_clean_body": ["thi", "add", "detail", "about", "version", "field", "both", "templat", "pipelin", "page"], "title_sim": [0.22047638470512665], "body_sim": [0.3985241991127709], "file_list_sim": 0.0, "overlap_files_len": 0, "code_sim": [0.24301318849682532, 0.0], "location_sim": [0.0, 0.0], "pattern": 0, "time": 9}, {"A_title": "Fixes parse error with complex shapes", "A_clean_title": ["fix", "pars", "error", "complex", "shape"], "B_title": "Only use dateline fix when polygon crosses dateline.", "B_clean_title": ["onli", "use", "datelin", "fix", "when", "polygon", "cross", "datelin"], "A_body": "The bug reproduces when the point under test for the placement of the hole of the polygon has an x coordinate which only intersects with the ends of edges in the main polygon. The previous code threw out these cases as not relevant but an intersect at 1.0 of the distance from the start to the end of an edge is just as valid as an intersect at any other point along the edge.  The fix corrects this and adds a test.\n\nCloses #5773\n", "A_clean_body": ["bug", "reproduc", "when", "point", "under", "test", "placement", "hole", "polygon", "ha", "coordin", "which", "onli", "intersect", "end", "edg", "main", "polygon", "previou", "code", "threw", "out", "these", "case", "as", "not", "relev", "but", "intersect", "at", "distanc", "start", "end", "edg", "just", "as", "valid", "as", "intersect", "at", "ani", "other", "point", "along", "edg", "fix", "correct", "thi", "add", "test", "close", "5773"], "B_body": "- es1.x attempts to split polygons on the dateline along\n  with holes, and then re-assign the holes to the new\n  polygons.  In certain circumstances this assignment\n  fails (https://github.com/elasticsearch/elasticsearch/issues/5773)\n  The geojson format makes it clear which hole belongs to which\n  polygon, and these issues can be avoided for polygons which\n  do not span the dateline.\n- there are additional issues with splitting polygons on the\n  dateline.  Some shapes are lost in certain circumstances (bug\n  yet to be filed).\n  Sample shape: https://gist.github.com/anonymous/7f1bb6d7e9cd72f5977c\n", "B_clean_body": ["es1", "attempt", "split", "polygon", "datelin", "along", "hole", "then", "re", "assign", "hole", "new", "polygon", "certain", "circumst", "thi", "assign", "fail", "http", "github", "com", "elasticsearch", "elasticsearch", "issu", "5773", "geojson", "format", "make", "it", "clear", "which", "hole", "belong", "which", "polygon", "these", "issu", "avoid", "polygon", "which", "not", "span", "datelin", "there", "are", "addit", "issu", "split", "polygon", "datelin", "some", "shape", "are", "lost", "certain", "circumst", "bug", "yet", "file", "sampl", "shape", "http", "github", "gist", "com", "anonym", "7f1bb6d7e9cd72f5977c"], "title_sim": [0.15008550455861538], "body_sim": [0.3435212619086127], "file_list_sim": 0.0, "overlap_files_len": 0, "code_sim": [0.026671019974967118, 0.0], "location_sim": [0.0, 0.0], "pattern": 1, "time": 70}, {"A_title": "remove build duplication", "A_clean_title": ["remov", "build", "duplic"], "B_title": "[maven] use elasticsearch-parent project", "B_clean_title": ["maven", "use", "elasticsearch", "parent", "project"], "A_body": "We've refactored the following logic into elasticsearch-parent, and its now being used by all plugins:\n- third party dependency versions\n- test configuration\n- compiler configuration (with minimum java version in one place)\n- build numbering / jar manifest metadata\n- forbidden-apis checks / definitions\n- license header checks / definitions\n- \"dev\" profile for skipping checks\n- ide configuration tasks and other minutae\n\nThere is more to do, like factoring out coverage analysis, PMD/findbugs and other static analysis, but this is a step I think we should make for overall consistency of builds.\n", "A_clean_body": ["we", "'ve", "refactor", "follow", "logic", "into", "elasticsearch", "parent", "it", "now", "be", "use", "by", "all", "plugin", "third", "parti", "depend", "version", "test", "configur", "compil", "configur", "minimum", "java", "version", "one", "place", "build", "number", "jar", "manifest", "metadata", "forbidden", "api", "check", "definit", "licens", "header", "check", "definit", "dev", "profil", "skip", "check", "ide", "configur", "task", "other", "minuta", "there", "more", "like", "factor", "out", "coverag", "analysi", "pmd", "findbug", "other", "static", "analysi", "but", "thi", "step", "think", "we", "make", "overal", "consist", "build"], "B_body": "We created another parent project [elasticsearch-parent](https://github.com/elasticsearch/elasticsearch-parent) which contains all dependencies that might be used in elasticsearch java projects including elasticsearch core itself.\n\nThis commit is the first step to this goal and could require some cleanup though `mvn clean install` is working fine.\n\nAlso, goal is to have in `elasticsearch-parent` one branch per major elasticsearch maintained version (master, 1.x) so this commit will be applied as well in 1.x branch but with a parent dependency on 1.5.0-SNAPSHOT.\n\nThe build release script must be also tested and modified if needed.\n\nIt might be interested to join the efforts done in plugins repos and in core repo to have at the end of the day one single release script available in `elasticsearch-common` project and that might be used as well by third-party plugins developers.\n\nNote that releasing an elasticsearch version will require first to release the `elasticsearch-parent` related version, then update elasticsearch `pom.xml` to depend on the final version (remove `-SNAPSHOT` that is).\n", "B_clean_body": ["we", "creat", "anoth", "parent", "project", "elasticsearch", "parent", "http", "parent", "github", "com", "elasticsearch", "elasticsearch", "which", "contain", "all", "depend", "that", "might", "use", "elasticsearch", "java", "project", "includ", "elasticsearch", "core", "itself", "thi", "commit", "first", "step", "thi", "goal", "could", "requir", "some", "cleanup", "though", "mvn", "clean", "instal", "work", "fine", "also", "goal", "have", "elasticsearch", "parent", "one", "branch", "per", "major", "elasticsearch", "maintain", "version", "master", "so", "thi", "commit", "will", "appli", "as", "well", "branch", "but", "parent", "depend", "snapshot", "build", "releas", "script", "must", "also", "test", "modifi", "need", "it", "might", "interest", "join", "effort", "done", "plugin", "repo", "core", "repo", "have", "at", "end", "day", "one", "singl", "releas", "script", "avail", "elasticsearch", "common", "project", "that", "might", "use", "as", "well", "by", "third", "parti", "plugin", "develop", "note", "that", "releas", "elasticsearch", "version", "will", "requir", "first", "releas", "elasticsearch", "parent", "relat", "version", "then", "updat", "elasticsearch", "pom", "xml", "depend", "final", "version", "remov", "snapshot", "that"], "title_sim": [-0.03645690489497488], "body_sim": [0.3363700820372951], "file_list_sim": 0, "overlap_files_len": 0, "code_sim": [0.0, 0.0], "location_sim": [0.0, 0.0], "pattern": 0, "time": 94}, {"A_title": "Execute Scripting Engine before searching for inner templates in template query", "A_clean_title": ["execut", "script", "engin", "befor", "search", "inner", "templat", "templat", "queri"], "B_title": "Search Template - conditional clauses not rendering correctly", "B_clean_title": ["search", "templat", "condit", "claus", "not", "render", "correctli"], "A_body": "The search template and template query did not run the template through the script engine before searching for an inner template. This meant that parsing for the inner template failed because the template was not always valid JSON (if it contained mustache code) when it was parsed to find the inner template. This has been fixed and Tests added to check for the failing behaviour\n\nTests are from https://github.com/elastic/elasticsearch/pull/8393\n", "A_clean_body": ["search", "templat", "templat", "queri", "did", "not", "run", "templat", "through", "script", "engin", "befor", "search", "inner", "templat", "thi", "meant", "that", "pars", "inner", "templat", "fail", "becaus", "templat", "wa", "not", "alway", "valid", "json", "it", "contain", "mustach", "code", "when", "it", "wa", "pars", "find", "inner", "templat", "thi", "ha", "been", "fix", "test", "ad", "check", "fail", "behaviour", "test", "are", "http", "github", "com", "elast", "elasticsearch", "pull", "8393"], "B_body": "- implemented the conditional parsing capabilities\n- attached few junit test cases to test it\n\nCloses #8308\n", "B_clean_body": ["implement", "condit", "pars", "capabl", "attach", "few", "junit", "test", "case", "test", "it", "close", "8308"], "title_sim": [0.49829072746907815], "body_sim": [0.19002827322549373], "file_list_sim": 0.0, "overlap_files_len": 0, "code_sim": [0.6147947821701575, 0.0], "location_sim": [0.0, 0.0], "pattern": -1, "time": 210}, {"A_title": "Better support for partial buffer reads/writes in translog infrastructure", "A_clean_title": ["better", "support", "partial", "buffer", "read", "write", "translog", "infrastructur"], "B_title": "Raise proper failure if not fully reading translog entry", "B_clean_title": ["rais", "proper", "failur", "not", "fulli", "read", "translog", "entri"], "A_body": "Some IO api can return after writing & reading only a part of the requested data. On these rare occasions, we should call the methods again to read/write the rest of the data. This has cause rare translog corruption while writing  huge documents on Windows.\n\nCloses #6441\n\nPS. I'll add java docs to the new Channels util class, but wanted to start the review process.\n", "A_clean_body": ["some", "io", "api", "return", "after", "write", "read", "onli", "part", "request", "data", "these", "rare", "occas", "we", "call", "method", "again", "read", "write", "rest", "data", "thi", "ha", "caus", "rare", "translog", "corrupt", "while", "write", "huge", "document", "window", "close", "6441", "ps", "'ll", "add", "java", "doc", "new", "channel", "util", "class", "but", "want", "start", "review", "process"], "B_body": "When reading a translog entry, raise a proper error when not reading it fully.\n", "B_clean_body": ["when", "read", "translog", "entri", "rais", "proper", "error", "when", "not", "read", "it", "fulli"], "title_sim": [0.4323778165831588], "body_sim": [0.24844530232490034], "file_list_sim": 0, "overlap_files_len": 0, "code_sim": [0.0, 0.0], "location_sim": [0.0, 0.0], "pattern": 0, "time": 1}, {"A_title": "Added a new script construct", "A_clean_title": ["ad", "new", "script", "construct"], "B_title": "Scripting: move ScriptType outside of ScriptService, to its own class file", "B_clean_title": ["script", "move", "scripttyp", "script", "type", "outsid", "scriptservic", "script", "servic", "it", "own", "class", "file"], "A_body": "Added an initial script construct to unify the parameters typically passed to\nmethods in the ScriptService. This changes the way several public\nmethods are called in the ScriptService along with all the callers since\nthey must wrap the parameters passed in into a script object. In the future,\nparsing parameters can also be moved into this construct along with ToXContent.\n", "A_clean_body": ["ad", "initi", "script", "construct", "unifi", "paramet", "typic", "pass", "method", "scriptservic", "script", "servic", "thi", "chang", "way", "sever", "public", "method", "are", "call", "scriptservic", "script", "servic", "along", "all", "caller", "sinc", "they", "must", "wrap", "paramet", "pass", "into", "script", "object", "futur", "pars", "paramet", "also", "move", "into", "thi", "construct", "along", "toxcont", "content"], "B_body": "Opening this up for discussion. I find it very weird that java api users have to import some `ScriptService` inner class when using scripts. `ScriptType` should belong to its own class, at the same level of `ScriptContext` (`org.elasticsearch.script` package).\n\nThe only problem with this change is that it breaks the Java api and plugins, as `ScriptType` needs to passed in as argument when using scripts. That is why the change is targeted for 2.0.\n", "B_clean_body": ["open", "thi", "up", "discuss", "find", "it", "veri", "weird", "that", "java", "api", "user", "have", "import", "some", "scriptservic", "script", "servic", "inner", "class", "when", "script", "scripttyp", "script", "type", "belong", "it", "own", "class", "at", "same", "level", "scriptcontext", "script", "context", "org", "elasticsearch", "script", "packag", "onli", "problem", "thi", "chang", "that", "it", "break", "java", "api", "plugin", "as", "scripttyp", "script", "type", "need", "pass", "as", "argument", "when", "script", "that", "whi", "chang", "target"], "title_sim": [0.4919715648071544], "body_sim": [0.6168375723898686], "file_list_sim": 0.36363636363636365, "overlap_files_len": 16, "code_sim": [0.05932509793363355, 0.0], "location_sim": [0.11975308641975309, 0.2934947049924357], "pattern": 0, "time": 9}, {"A_title": "Enable SSL for Azure blob storage ", "A_clean_title": ["enabl", "ssl", "azur", "blob", "storag"], "B_title": "Use https to talk to blob.core.windows.com", "B_clean_title": ["use", "http", "talk", "blob", "core", "window", "com"], "A_body": "In order to increase Security.\n\nThe Microsoft Azure storage services support both HTTP and HTTPS; however, using HTTPS is highly recommended.\n", "A_clean_body": ["order", "increas", "secur", "microsoft", "azur", "storag", "servic", "support", "both", "http", "http", "howev", "http", "highli", "recommend"], "B_body": "... because security and stuff!\n", "B_clean_body": ["becaus", "secur", "stuff"], "title_sim": [0.11514427048508699], "body_sim": [0.3616777773219891], "file_list_sim": 1.0, "overlap_files_len": 1, "code_sim": [0.0, 0.0], "location_sim": [1.0, 1.0], "pattern": 0, "time": 4}, {"A_title": "Made template filtering generic and extensible", "A_clean_title": ["made", "templat", "filter", "gener", "extens"], "B_title": "Introduced IndexTemplateFilter", "B_clean_title": ["introduc", "indextemplatefilt", "index", "templat", "filter"], "A_body": "Added the ability to register index template filters that are being applied when a new index is created, in order to decide whether a matching index template should be applied or not. The default filter that checks whether the template pattern matches the index name always runs first, additional filters can also be registered so that templates can be filtered out based on custom logic.\n\nTook the chance to add the handy `source(Object... source)` method to `PutIndexTemplateRequest` and corresponding builder.\n\nCloses #7459\n", "A_clean_body": ["ad", "abil", "regist", "index", "templat", "filter", "that", "are", "be", "appli", "when", "new", "index", "creat", "order", "decid", "whether", "match", "index", "templat", "appli", "or", "not", "default", "filter", "that", "check", "whether", "templat", "pattern", "match", "index", "name", "alway", "run", "first", "addit", "filter", "also", "regist", "so", "that", "templat", "filter", "out", "base", "custom", "logic", "took", "chanc", "add", "handi", "sourc", "object", "sourc", "method", "putindextemplaterequest", "put", "index", "templat", "request", "correspond", "builder", "close", "7459"], "B_body": "Added the ability to register filters on the index templates that are being applied when a new index is created. These filters will be able to filter out templates that should not be applied to newly created index.\n", "B_clean_body": ["ad", "abil", "regist", "filter", "index", "templat", "that", "are", "be", "appli", "when", "new", "index", "creat", "these", "filter", "will", "abl", "filter", "out", "templat", "that", "not", "appli", "newli", "creat", "index"], "title_sim": [0.6252511589952662], "body_sim": [0.8019200628220617], "file_list_sim": 0.6666666666666666, "overlap_files_len": 6, "code_sim": [0.8372181116780861, 0.8408758541359269], "location_sim": [0.6989853438556933, 0.7673267326732673], "pattern": 0, "time": 3}, {"A_title": "Separate cluster update tasks that are published from those that are not", "A_clean_title": ["separ", "cluster", "updat", "task", "that", "are", "publish", "those", "that", "are", "not"], "B_title": "Non-master node cannot commit changes to the cluster state", "B_clean_title": ["non", "master", "node", "not", "commit", "chang", "cluster", "state"], "A_body": "This PR factors out the cluster state update tasks that are published to the nodes from those that are not, serving as a basis for future refactorings to separate the publishing mechanism out of cluster service.", "A_clean_body": ["thi", "pr", "factor", "out", "cluster", "state", "updat", "task", "that", "are", "publish", "node", "those", "that", "are", "not", "serv", "as", "basi", "futur", "refactor", "separ", "publish", "mechan", "out", "cluster", "servic"], "B_body": "There was only one remaining place where a non-master node would\r\nalter the cluster state on its own local node - to update the\r\nNO_MASTER cluster block.  In order to ensure that the authoritative\r\ncluster state on each node is only updated by the master node, this\r\ncommit removes updating the NO_MASTER block in the cluster state via\r\na cluster state update task.  Instead, the ClusterService now uses\r\nthe newly created ClusterServiceState (see #21379) to maintain the\r\ncurrent cluster state and the local view of the cluster, including\r\nwhether there is currently a master from the point of view of the\r\nClusterService.  In order to minimize the impact of this change,\r\nwhenever the ClusterService#state() method is invoked to retrieve\r\nthe current state, if the no master flag is set, the returned\r\ncluster state will contain a NO_MASTER block overlayed on the\r\nauthoritative state, without altering the authoritative cluster state.", "B_clean_body": ["there", "wa", "onli", "one", "remain", "place", "where", "non", "master", "node", "would", "alter", "cluster", "state", "it", "own", "local", "node", "updat", "no", "master", "cluster", "block", "order", "ensur", "that", "authorit", "cluster", "state", "each", "node", "onli", "updat", "by", "master", "node", "thi", "commit", "remov", "updat", "no", "master", "block", "cluster", "state", "via", "cluster", "state", "updat", "task", "instead", "clusterservic", "cluster", "servic", "now", "use", "newli", "creat", "clusterservicest", "cluster", "servic", "state", "see", "21379", "maintain", "current", "cluster", "state", "local", "view", "cluster", "includ", "whether", "there", "current", "master", "point", "view", "clusterservic", "cluster", "servic", "order", "minim", "impact", "thi", "chang", "whenev", "clusterservic", "cluster", "servic", "state", "method", "invok", "retriev", "current", "state", "no", "master", "flag", "set", "return", "cluster", "state", "will", "contain", "no", "master", "block", "overlay", "authorit", "state", "without", "alter", "authorit", "cluster", "state"], "title_sim": [0.2731704834363492], "body_sim": [0.5388923776167328], "file_list_sim": 0, "overlap_files_len": 0, "code_sim": [0.0, 0.0], "location_sim": [0.0, 0.0], "pattern": 0, "time": 21}, {"A_title": "Remove support for new indexes using `path` setting in object/nested fields or `index_name` in any field", "A_clean_title": ["remov", "support", "new", "index", "path", "set", "object", "nest", "field", "or", "index", "name", "ani", "field"], "B_title": "Mappings: Deprecate `index_name`.", "B_clean_title": ["map", "deprec", "index", "name"], "A_body": "Backcompat is still here for indexes created before 2.0. We can remove this backcompat when/if we implement mappings migration capabilities as @clintongormley described in #6677.\n\ncloses #6677\n\nFor reviewers: I did not conditionalize the writing of \"path\" because the value will always be the default as long as backcompat exists, and there is currently nothing written in the case of \"include_defaults\" (seems like an existing problem).  I can fix this if anyone feels strongly about it, but I opted to leave it alone since we are removing the setting anyways.\n", "A_clean_body": ["backcompat", "still", "here", "index", "creat", "befor", "we", "remov", "thi", "backcompat", "when", "we", "implement", "map", "migrat", "capabl", "as", "clintongormley", "describ", "6677", "close", "6677", "review", "did", "not", "condition", "write", "path", "becaus", "valu", "will", "alway", "default", "as", "long", "as", "backcompat", "exist", "there", "current", "noth", "written", "case", "includ", "default", "seem", "like", "exist", "problem", "fix", "thi", "anyon", "feel", "strongli", "about", "it", "but", "opt", "leav", "it", "alon", "sinc", "we", "are", "remov", "set", "anyway"], "B_body": "Close #6677\n", "B_clean_body": ["close", "6677"], "title_sim": [0.38185895806758086], "body_sim": [0.10314117090309347], "file_list_sim": 0.5, "overlap_files_len": 15, "code_sim": [0.26161644416544566, 0.3582526086526096], "location_sim": [0.45495495495495497, 0.753731343283582], "pattern": 1, "time": 191}, {"A_title": "Upgrade checkstyle to version 7.5", "A_clean_title": ["upgrad", "checkstyl", "version"], "B_title": "Update checkstyle and fix violations", "B_clean_title": ["updat", "checkstyl", "fix", "violat"], "A_body": "This commit upgrades the checkstyle configuration from version 5.9 to\r\nversion 7.5, the latest version as of today. The main enhancement\r\nobtained via this upgrade is better detection of redundant modifiers.\r\n", "A_clean_body": ["thi", "commit", "upgrad", "checkstyl", "configur", "version", "version", "latest", "version", "as", "today", "main", "enhanc", "obtain", "via", "thi", "upgrad", "better", "detect", "redund", "modifi"], "B_body": "This PR updates the version of Checkstyle from current `5.9` to `7.1` and fix a bunch of new violations.\n\nA more recent version would allow us to use some new checkstyle modules (like [Javadocs](http://checkstyle.sourceforge.net/config_javadoc.html)). \n\n`RedundantModifier` is improved and now detects more violations that are fixed by this PR. The changes fall into three categories (thanks @nik9000 for the nice summary):\n- `public` methods on non-`public` classes don't make sense. Remove the modifier and let the method inherit the modifier.\n- try-with-resources variables should not be declared `final` because they are implicitly `final`. `final` adds no extra information for the reader. I guess that makes them names rather than variables....\n- `enum`s are alway `static` so there is no need to declare them `static`.\n- `final` doesn't do anything on anonymous classes because they can't be extended anyway. So we just remove the modifier.\n- Classes declared inside of interfaces are always `static` so we shouldn't also declare them as `static`.\n\nFor the record, most of the changes were automated using this small Bash script:\n\n```\n#!/bin/bash\ninput=\"/tmp/violations.txt\"\n\n# File is composed of lines like:\n# /tmp/elasticsearch/src/test/java/org/elasticsearch/action/Action.java:10:9:public\n# \nwhile IFS=':' read -ra ADDR; do\n    if [ \"${ADDR[3]}\" != \"\" ]; then\n        echo \"${ADDR[0]}\" | xargs sed -i \"${ADDR[1]}s/${ADDR[3]} //\"\n    else \n        echo \"Skipping file ${ADDR[0]} line ${ADDR[1]} word ${ADDR[3]}\"\n    fi\ndone < \"$input\"\n```\n\nCheckstyle was not updated to the latest version 7.1.2 because it reports violations for `final` parameters in interface methods (checkstyle/checkstyle#3322) and we have a lot of these for good reason.\n\nI'm really sorry for the boringness of this kind of pull request.\n", "B_clean_body": ["thi", "pr", "updat", "version", "checkstyl", "current", "fix", "bunch", "new", "violat", "more", "recent", "version", "would", "allow", "us", "use", "some", "new", "checkstyl", "modul", "like", "javadoc", "http", "sourceforg", "html", "checkstyl", "javadoc", "net", "config", "redundantmodifi", "redund", "modifi", "improv", "now", "detect", "more", "violat", "that", "are", "fix", "by", "thi", "pr", "chang", "fall", "into", "three", "categori", "thank", "nik9000", "nice", "summari", "public", "method", "non", "public", "class", "n't", "make", "sens", "remov", "modifi", "let", "method", "inherit", "modifi", "tri", "resourc", "variabl", "not", "declar", "final", "becaus", "they", "are", "implicitli", "final", "final", "add", "no", "extra", "inform", "reader", "guess", "that", "make", "them", "name", "rather", "than", "variabl", "enum", "are", "alway", "static", "so", "there", "no", "need", "declar", "them", "static", "final", "n't", "anyth", "anonym", "class", "becaus", "they", "ca", "n't", "extend", "anyway", "so", "we", "just", "remov", "modifi", "class", "declar", "insid", "interfac", "are", "alway", "static", "so", "we", "n't", "also", "declar", "them", "as", "static", "record", "most", "chang", "were", "autom", "thi", "small", "bash", "script", "bin", "bash", "input=", "txt", "tmp", "violat", "file", "compos", "line", "like", "java:10:9", "tmp", "elasticsearch", "src", "test", "java", "org", "elasticsearch", "action", "action", "public", "while", "ifs=", "read", "ra", "addr", "addr", "then", "echo", "addr", "xarg", "sed", "addr", "addr", "echo", "skip", "file", "addr", "line", "addr", "word", "addr", "fi", "done", "input", "checkstyl", "wa", "not", "updat", "latest", "version", "becaus", "it", "report", "violat", "final", "paramet", "interfac", "method", "checkstyl", "checkstyl", "3322", "we", "have", "lot", "these", "good", "reason", "'m", "realli", "sorri", "boring", "thi", "kind", "pull", "request"], "title_sim": [0.012510768077777394], "body_sim": [0.3190468619180953], "file_list_sim": 0, "overlap_files_len": 0, "code_sim": [0.0, 0.0], "location_sim": [0.0, 0.0], "pattern": 1, "time": 113}, {"A_title": "Mapping check during phase2 should be done in cluster state update task", "A_clean_title": ["map", "check", "dure", "phase2", "done", "cluster", "state", "updat", "task"], "B_title": "Mapping: Fix delete mapping race condition.", "B_clean_title": ["map", "fix", "delet", "map", "race", "condit"], "A_body": "Before phase2 of the recovery process, we check verify that the local mapping is in sync with the cluster state mapping (and send & wait on a master update mapping task if not). This check should be done under a cluster state update task to make sure an incoming cluster state update to do not change things while we check.\n", "A_clean_body": ["befor", "phase2", "recoveri", "process", "we", "check", "verifi", "that", "local", "map", "sync", "cluster", "state", "map", "send", "wait", "master", "updat", "map", "task", "not", "thi", "check", "done", "under", "cluster", "state", "updat", "task", "make", "sure", "incom", "cluster", "state", "updat", "not", "chang", "thing", "while", "we", "check"], "B_body": "If multiple clients attempted to delete a mapping, or a single client attempted to delete a mapping as an index is being\ncreated a NPE could be observed in the MetaDataMappingService. This fix makes sure we don't try to access a null indexMetaData object.\nAlso add a test to spawn multiple create and delete threads to provoke this race condition.\n\nCloses #5997\n", "B_clean_body": ["multipl", "client", "attempt", "delet", "map", "or", "singl", "client", "attempt", "delet", "map", "as", "index", "be", "creat", "npe", "could", "observ", "metadatamappingservic", "meta", "data", "map", "servic", "thi", "fix", "make", "sure", "we", "n't", "tri", "access", "null", "indexmetadata", "index", "meta", "data", "object", "also", "add", "test", "spawn", "multipl", "creat", "delet", "thread", "provok", "thi", "race", "condit", "close", "5997"], "title_sim": [0.2882941462729328], "body_sim": [0.21058466714823917], "file_list_sim": 0.0, "overlap_files_len": 0, "code_sim": [0.09226656362527087, 0.0], "location_sim": [0.0, 0.0], "pattern": 0, "time": 91}, {"A_title": "Mark id as required for delete_template", "A_clean_title": ["mark", "id", "as", "requir", "delet", "templat"], "B_title": "id route value is required", "B_clean_title": ["id", "rout", "valu", "requir"], "A_body": "", "A_clean_body": [], "B_body": "id route value is required\n", "B_clean_body": ["id", "rout", "valu", "requir"], "title_sim": [0.4264487202639165], "body_sim": [0.0], "file_list_sim": 1.0, "overlap_files_len": 1, "code_sim": [0.0, 0.0], "location_sim": [1.0, 1.0], "pattern": 0, "time": 7}, {"A_title": "If we can't get a MAC address for the node, use a dummy one", "A_clean_title": ["we", "ca", "n't", "get", "mac", "address", "node", "use", "dummi", "one"], "B_title": "Catch java.lang.Error when getting a mac address", "B_clean_title": ["catch", "java", "lang", "error", "when", "get", "mac", "address"], "A_body": "We already use a dummy MAC address if we hit a `SocketException` while trying; this PR just widens that to any `Throwable`.\n\nCloses #10099 \n", "A_clean_body": ["we", "alreadi", "use", "dummi", "mac", "address", "we", "hit", "socketexcept", "socket", "except", "while", "tri", "thi", "pr", "just", "widen", "that", "ani", "throwabl", "close", "10099"], "B_body": "Apparently Azure WebJobs throw a java.lang.Error when enumerating network interfaces.\nCatch this error and just use the generated one in this case.\n\nSee #10099\n", "B_clean_body": ["appar", "azur", "webjob", "web", "job", "throw", "java", "lang", "error", "when", "enumer", "network", "interfac", "catch", "thi", "error", "just", "use", "gener", "one", "thi", "case", "see", "10099"], "title_sim": [0.25318881590899467], "body_sim": [0.19885283613862542], "file_list_sim": 0.0, "overlap_files_len": 0, "code_sim": [0.0, 0.0], "location_sim": [0.0, 0.0], "pattern": 1, "time": 265}, {"A_title": "Cleanup local code transport execution", "A_clean_title": ["cleanup", "local", "code", "transport", "execut"], "B_title": "Remove unused code from TransportShardReplicationOperationAction", "B_clean_title": ["remov", "unus", "code", "transportshardreplicationoperationact", "transport", "shard", "replic", "oper", "action"], "A_body": "Now that we handle automatically the local execution within the transport service, we can remove parts of the code that handle it in actions.\n", "A_clean_body": ["now", "that", "we", "handl", "automat", "local", "execut", "within", "transport", "servic", "we", "remov", "part", "code", "that", "handl", "it", "action"], "B_body": "This code can only be executed if primary and replica were on the same node\nin which case an exception or something of the likes would be more appropriate. \nI still find it a little tricky to figure out how replica requests end up in the indexing threadpool and will try to make a pr for that as well.\n", "B_clean_body": ["thi", "code", "onli", "execut", "primari", "replica", "were", "same", "node", "which", "case", "except", "or", "someth", "like", "would", "more", "appropri", "still", "find", "it", "littl", "tricki", "figur", "out", "how", "replica", "request", "end", "up", "index", "threadpool", "will", "tri", "make", "pr", "that", "as", "well"], "title_sim": [0.27037385631528443], "body_sim": [0.1863972077368461], "file_list_sim": 0, "overlap_files_len": 0, "code_sim": [0.0, 0.0], "location_sim": [0.0, 0.0], "pattern": 0, "time": 4}, {"A_title": "Fix automatic installation of plugin for integ tests", "A_clean_title": ["fix", "automat", "instal", "plugin", "integ", "test"], "B_title": "IT for plugins should use plugin name and not artifactId", "B_clean_title": ["it", "plugin", "use", "plugin", "name", "not", "artifactid", "artifact", "id"], "A_body": "The esplugin gradle plugin automatically adds the pluging being built to the integTest cluster. However, there were two issues with this. First was a bug in the name, which should have been the configured\nesplugin.name instead of the project name. Second, the files configuration was overcomplicated (trying to use the groovy spreader operator after delaying calls to singleFile). Instead, we can just pass the file collections (which will just be a single file at execution time).\n", "A_clean_body": ["esplugin", "gradl", "plugin", "automat", "add", "pluge", "be", "built", "integtest", "integ", "test", "cluster", "howev", "there", "were", "two", "issu", "thi", "first", "wa", "bug", "name", "which", "have", "been", "configur", "esplugin", "name", "instead", "project", "name", "second", "file", "configur", "wa", "overcompl", "tri", "use", "groovi", "spreader", "oper", "after", "delay", "call", "singlefil", "singl", "file", "instead", "we", "just", "pass", "file", "collect", "which", "will", "just", "singl", "file", "at", "execut", "time"], "B_body": "In mapper attachments plugin, we needed to change the default value for `elasticsearch.plugin.name`. It was by default `artifactId` (elasticsearch-mapper-attachments) but it should have been `mapper-attachments`.\n\nSo we changed in `pom.xml`\n\n```\n<elasticsearch.plugin.name>mapper-attachments</elasticsearch.plugin.name>\n```\n\nThe problem is that integration tests expect to install a plugin which name is the `project.artifactId` and not the `elasticsearch.plugin.name`.\n\nThis commit fixes that.\n\nNote that I have no idea on how to fix that in master branch. So I propose for now to push this in 2.x, 2.1 and 2.0 branches.\n", "B_clean_body": ["mapper", "attach", "plugin", "we", "need", "chang", "default", "valu", "elasticsearch", "plugin", "name", "it", "wa", "by", "default", "artifactid", "artifact", "id", "elasticsearch", "mapper", "attach", "but", "it", "have", "been", "mapper", "attach", "so", "we", "chang", "pom", "xml", "elasticsearch", "plugin", "name", "mapper", "attach", "plugin", "name", "elasticsearch", "problem", "that", "integr", "test", "expect", "instal", "plugin", "which", "name", "project", "artifactid", "artifact", "id", "not", "elasticsearch", "plugin", "name", "thi", "commit", "fix", "that", "note", "that", "have", "no", "idea", "how", "fix", "that", "master", "branch", "so", "propos", "now", "push", "thi", "branch"], "title_sim": [0.4092988364497663], "body_sim": [0.37255026660308355], "file_list_sim": 0.0, "overlap_files_len": 0, "code_sim": [0.0, 0.0], "location_sim": [0.0, 0.0], "pattern": 0, "time": 3}, {"A_title": "Don't write bad search requests into exception messages", "A_clean_title": ["n't", "write", "bad", "search", "request", "into", "except", "messag"], "B_title": "Cleaner query parse error feedback", "B_clean_title": ["cleaner", "queri", "pars", "error", "feedback"], "A_body": "See issue #8370.\n\nIf a search requests's source doesn't parse, log the\nsource instead of putting it into an exception\nmessage and propagating it. This prevents large\nstrings from being built up.\n\nIdeally the source could be streamed to a log file\nso that it never has to be rendered in-memory in\nits entirety, but this is a good start.\n", "A_clean_body": ["see", "issu", "8370", "search", "request", "'s", "sourc", "n't", "pars", "log", "sourc", "instead", "put", "it", "into", "except", "messag", "propag", "it", "thi", "prevent", "larg", "string", "be", "built", "up", "ideal", "sourc", "could", "stream", "log", "file", "so", "that", "it", "never", "ha", "render", "memori", "it", "entireti", "but", "thi", "good", "start"], "B_body": "Provides the line and column number in user input where errors were spotted and a succinct parse error message, free of server-side stack trace info.\n\nMany \"*parser\" classes are changed but the core framework changes are:\n\n1) XContentParser can now return a new \"XContentLocation\" object describing position of last token parsed\n2) Existing SearchParseException and QueryParsingException classes have all constructors changed to take an XContentLocation object to encourage supply of detailed parse feedback\n3) Common \"UserInputException\" interface introduced for exceptions relating to user input (the 2 above).\n4) SearchPhaseExecutionException that gathers results from many shards holds onto only the first of several (typically duplicate) UserInputExceptions from shards\n5) BytesRestResponse outputs extra JSON fields for parse failures including line, column and succinct error message.\n\nCloses #3303\n", "B_clean_body": ["provid", "line", "column", "number", "user", "input", "where", "error", "were", "spot", "succinct", "pars", "error", "messag", "free", "server", "side", "stack", "trace", "info", "mani", "*parser", "class", "are", "chang", "but", "core", "framework", "chang", "are", "xcontentpars", "content", "parser", "now", "return", "new", "xcontentloc", "content", "locat", "object", "describ", "posit", "last", "token", "pars", "exist", "searchparseexcept", "search", "pars", "except", "queryparsingexcept", "queri", "pars", "except", "class", "have", "all", "constructor", "chang", "take", "xcontentloc", "content", "locat", "object", "encourag", "suppli", "detail", "pars", "feedback", "common", "userinputexcept", "user", "input", "except", "interfac", "introduc", "except", "relat", "user", "input", "abov", "searchphaseexecutionexcept", "search", "phase", "execut", "except", "that", "gather", "result", "mani", "shard", "hold", "onto", "onli", "first", "sever", "typic", "duplic", "userinputexcept", "user", "input", "except", "shard", "bytesrestrespons", "byte", "rest", "respons", "output", "extra", "json", "field", "pars", "failur", "includ", "line", "column", "succinct", "error", "messag", "close", "3303"], "title_sim": [0.011350382399545143], "body_sim": [0.2829061171991633], "file_list_sim": 0, "overlap_files_len": 0, "code_sim": [0.0, 0.0], "location_sim": [0.0, 0.0], "pattern": -1, "time": 41}, {"A_title": "Add short_circuit option to limit filters, triggering a shard search failure on overly broad searches", "A_clean_title": ["add", "short", "circuit", "option", "limit", "filter", "trigger", "shard", "search", "failur", "overli", "broad", "search"], "B_title": "A new generic timeout handler and changes to existing search classes to use it", "B_clean_title": ["new", "gener", "timeout", "handler", "chang", "exist", "search", "class", "use", "it"], "A_body": "Proposing this to get a discussion started on how to identify when a limit filter is dropping potentially valid matching documents.\n\nThe case I'm attempting to tackle occurs when a user makes a broad search with high computational complexity, say a script filter.  What I need is a mechanism to be able to terminate searches on shards after exceeding a certain number of filtered documents.  A better timeout mechanism would also help to alleviate this problem.\n\nLimit filters are currently the best means to restrict expensive searches, but unfortunately there is no context returned to identify when they are tripped.  This makes it impossible to know when to inform a user that their query was too broad vs. trust that there are in fact no matches.\n\nI see throwing an unchecked exception as a massive kludge, but I don't see a much better option to handle this sort of situation.  Any feedback/recommendations welcome.\n", "A_clean_body": ["propos", "thi", "get", "discuss", "start", "how", "identifi", "when", "limit", "filter", "drop", "potenti", "valid", "match", "document", "case", "'m", "attempt", "tackl", "occur", "when", "user", "make", "broad", "search", "high", "comput", "complex", "say", "script", "filter", "what", "need", "mechan", "abl", "termin", "search", "shard", "after", "exceed", "certain", "number", "filter", "document", "better", "timeout", "mechan", "would", "also", "help", "allevi", "thi", "problem", "limit", "filter", "are", "current", "best", "mean", "restrict", "expens", "search", "but", "unfortun", "there", "no", "context", "return", "identifi", "when", "they", "are", "trip", "thi", "make", "it", "imposs", "know", "when", "inform", "user", "that", "their", "queri", "wa", "too", "broad", "vs", "trust", "that", "there", "are", "fact", "no", "match", "see", "throw", "uncheck", "except", "as", "massiv", "kludg", "but", "n't", "see", "much", "better", "option", "handl", "thi", "sort", "situat", "ani", "feedback", "recommend", "welcom"], "B_body": "A more effective approach to time-limiting activities such as search requests. Special runtime exceptions can now short-cut the execution of long-running calls to Lucene classes and are caught and reported back, not as a fatal error but using the existing \u201ctimedOut\u201d flag in results.\n\nPhases like the FetchPhase can now exit early and so also have a timed-out status. The SearchPhaseController does its best to assemble whatever hits, aggregations and facets have been produced within the provided time limits rather than returning nothing and throwing an error.\n\nActivityTimeMonitor is the new central class for efficiently monitoring all forms of thread overrun in a JVM.\nThe SearchContext setup is modified to register the start and end of query tasks with ActivityTimeMonitor.\nStore.java is modified to add timeout checks (via calls to ATM) in the low-level file access routines by using a delegating wrapper for Lucene's IndexInput and IndexInputSlicer.\nContextIndexSearcher is modified to catch and unwrap ActivityTimedOutExceptions that can now come out of the Lucene calls and report them as timeouts along with any partial results.\nFetchPhase is similarly modified to deal with the possibility of timeout errors.\n", "B_clean_body": ["more", "effect", "approach", "time", "limit", "activ", "such", "as", "search", "request", "special", "runtim", "except", "now", "short", "cut", "execut", "long", "run", "call", "lucen", "class", "are", "caught", "report", "back", "not", "as", "fatal", "error", "but", "exist", "timedout", "time", "out", "flag", "result", "phase", "like", "fetchphas", "fetch", "phase", "now", "exit", "earli", "so", "also", "have", "time", "out", "statu", "searchphasecontrol", "search", "phase", "control", "it", "best", "assembl", "whatev", "hit", "aggreg", "facet", "have", "been", "produc", "within", "provid", "time", "limit", "rather", "than", "return", "noth", "throw", "error", "activitytimemonitor", "activ", "time", "monitor", "new", "central", "class", "effici", "monitor", "all", "form", "thread", "overrun", "jvm", "searchcontext", "search", "context", "setup", "modifi", "regist", "start", "end", "queri", "task", "activitytimemonitor", "activ", "time", "monitor", "store", "java", "modifi", "add", "timeout", "check", "via", "call", "atm", "low", "level", "file", "access", "routin", "by", "deleg", "wrapper", "lucen", "'s", "indexinput", "index", "input", "indexinputslic", "index", "input", "slicer", "contextindexsearch", "context", "index", "searcher", "modifi", "catch", "unwrap", "activitytimedoutexcept", "activ", "time", "out", "except", "that", "now", "come", "out", "lucen", "call", "report", "them", "as", "timeout", "along", "ani", "partial", "result", "fetchphas", "fetch", "phase", "similarli", "modifi", "deal", "possibl", "timeout", "error"], "title_sim": [0.2545178674426841], "body_sim": [0.24227466915885237], "file_list_sim": 0, "overlap_files_len": 0, "code_sim": [0.0, 0.0], "location_sim": [0.0, 0.0], "pattern": 0, "time": 141}, {"A_title": "Aggregations: Fixed DateHistogramBuilder to accept preOffset and postOff...", "A_clean_title": ["aggreg", "fix", "datehistogrambuild", "date", "histogram", "builder", "accept", "preoffset", "pre", "offset", "postoff", "post", "off"], "B_title": "Fix DateHistogramBuilder to use a String pre_offset and post_offset", "B_clean_title": ["fix", "datehistogrambuild", "date", "histogram", "builder", "use", "string", "pre", "offset", "post", "offset"], "A_body": "...set as Strings\n\nThis is what DateHistogramParser expects so will enable the builder to build valid requests using these variables.\nAlso added tests for preOffset and postOffset since these tests did not exist\n\nCloses #5586\n", "A_clean_body": ["set", "as", "string", "thi", "what", "datehistogrampars", "date", "histogram", "parser", "expect", "so", "will", "enabl", "builder", "build", "valid", "request", "these", "variabl", "also", "ad", "test", "preoffset", "pre", "offset", "postoffset", "post", "offset", "sinc", "these", "test", "did", "not", "exist", "close", "5586"], "B_body": "Currently, if `preOffset` or `postOffset` are used in the `DateHistogramBuilder`, the generated query fails parsing in the `DateHistogramParser`.\n\nCloses #5586\n", "B_clean_body": ["current", "preoffset", "pre", "offset", "or", "postoffset", "post", "offset", "are", "use", "datehistogrambuild", "date", "histogram", "builder", "gener", "queri", "fail", "pars", "datehistogrampars", "date", "histogram", "parser", "close", "5586"], "title_sim": [0.795049193371572], "body_sim": [0.5735307825611612], "file_list_sim": 1.0, "overlap_files_len": 2, "code_sim": [0.9969183731040137, 0.9969183731040137], "location_sim": [1.0, 1.0], "pattern": 1, "time": 104}, {"A_title": "adding Service header for systemd override file", "A_clean_title": ["ad", "servic", "header", "systemd", "overrid", "file"], "B_title": "add section header to systemd unit extension", "B_clean_title": ["add", "section", "header", "systemd", "unit", "extens"], "A_body": "<!--\r\nThank you for your interest in and contributing to Elasticsearch! There\r\nare a few simple things to check before submitting your pull request\r\nthat can help with the review process. You should delete these items\r\nfrom your submission, but they are here to help bring them to your\r\nattention.\r\n-->\r\n\r\n- Have you signed the [contributor license agreement](https://www.elastic.co/contributor-agreement)?\r\n- Have you followed the [contributor guidelines](https://github.com/elastic/elasticsearch/blob/master/CONTRIBUTING.md)?\r\n- If submitting code, have you built your formula locally prior to submission with `gradle check`?\r\n- If submitting code, is your pull request against master? Unless there is a good reason otherwise, we prefer pull requests against master and will backport as needed.\r\n- If submitting code, have you checked that your submission is for an [OS that we support](https://www.elastic.co/support/matrix#show_os)?\r\n\r\nIt will not work under ubuntu 16.04 without the `[Service]` header in the systemd override conf.", "A_clean_body": ["thank", "you", "your", "interest", "contribut", "elasticsearch", "there", "are", "few", "simpl", "thing", "check", "befor", "submit", "your", "pull", "request", "that", "help", "review", "process", "you", "delet", "these", "item", "your", "submiss", "but", "they", "are", "here", "help", "bring", "them", "your", "attent", "have", "you", "sign", "contributor", "licens", "agreement", "http", "agreement", "elast", "www", "co", "contributor", "have", "you", "follow", "contributor", "guidelin", "http", "md", "github", "com", "elast", "elasticsearch", "blob", "master", "contribut", "submit", "code", "have", "you", "built", "your", "formula", "local", "prior", "submiss", "gradl", "check", "submit", "code", "your", "pull", "request", "against", "master", "unless", "there", "good", "reason", "otherwis", "we", "prefer", "pull", "request", "against", "master", "will", "backport", "as", "need", "submit", "code", "have", "you", "check", "that", "your", "submiss", "os", "that", "we", "support", "http", "elast", "www", "co", "support", "matrix", "show", "os", "it", "will", "not", "work", "under", "ubuntu", "16", "04", "without", "servic", "header", "systemd", "overrid", "conf"], "B_body": "<!--\r\nThank you for your interest in and contributing to Elasticsearch! There\r\nare a few simple things to check before submitting your pull request\r\nthat can help with the review process. You should delete these items\r\nfrom your submission, but they are here to help bring them to your\r\nattention.\r\n-->\r\n\r\n- Have you signed the [contributor license agreement](https://www.elastic.co/contributor-agreement)?\r\n- Have you followed the [contributor guidelines](https://github.com/elastic/elasticsearch/blob/master/CONTRIBUTING.md)?\r\n- If submitting code, have you built your formula locally prior to submission with `gradle check`?\r\n- If submitting code, is your pull request against master? Unless there is a good reason otherwise, we prefer pull requests against master and will backport as needed.\r\n- If submitting code, have you checked that your submission is for an [OS that we support](https://www.elastic.co/support/matrix#show_os)?\r\n\r\nWithout [Service] in the /etc/systemd/system/elasticsearch.service.d/elasticsearch.conf file the following error is shown and the setting is not used:\r\n````\r\nNov 21 14:14:03 es50-eb-5bj1g-1 systemd[1]: [/etc/systemd/system/elasticsearch.service.d/elasticsearch.conf:1] Assignment outside of section. Ignoring.\r\n````", "B_clean_body": ["thank", "you", "your", "interest", "contribut", "elasticsearch", "there", "are", "few", "simpl", "thing", "check", "befor", "submit", "your", "pull", "request", "that", "help", "review", "process", "you", "delet", "these", "item", "your", "submiss", "but", "they", "are", "here", "help", "bring", "them", "your", "attent", "have", "you", "sign", "contributor", "licens", "agreement", "http", "agreement", "elast", "www", "co", "contributor", "have", "you", "follow", "contributor", "guidelin", "http", "md", "github", "com", "elast", "elasticsearch", "blob", "master", "contribut", "submit", "code", "have", "you", "built", "your", "formula", "local", "prior", "submiss", "gradl", "check", "submit", "code", "your", "pull", "request", "against", "master", "unless", "there", "good", "reason", "otherwis", "we", "prefer", "pull", "request", "against", "master", "will", "backport", "as", "need", "submit", "code", "have", "you", "check", "that", "your", "submiss", "os", "that", "we", "support", "http", "elast", "www", "co", "support", "matrix", "show", "os", "without", "servic", "servic", "conf", "etc", "systemd", "system", "elasticsearch", "elasticsearch", "file", "follow", "error", "shown", "set", "not", "use", "nov", "21", "14:14:03", "es50", "eb", "5bj1g", "systemd", "servic", "conf:1", "etc", "systemd", "system", "elasticsearch", "elasticsearch", "assign", "outsid", "section", "ignor"], "title_sim": [0.6112359139515918], "body_sim": [0.9513791695872293], "file_list_sim": 1.0, "overlap_files_len": 1, "code_sim": [1.0, 1.0], "location_sim": [1.0, 1.0], "pattern": 0, "time": 16}, {"A_title": "Tribe nodes should apply cluster state updates in batches", "A_clean_title": ["tribe", "node", "appli", "cluster", "state", "updat", "batch"], "B_title": "TribeNode: batch processing of cluster states", "B_clean_title": ["tribenod", "tribe", "node", "batch", "process", "cluster", "state"], "A_body": "This commit applies the general mechanism for applying cluster state updates in batches to tribe nodes.\n\nRelates #14899, relates #14725\n", "A_clean_body": ["thi", "commit", "appli", "gener", "mechan", "appli", "cluster", "state", "updat", "batch", "tribe", "node", "relat", "14899", "relat", "14725"], "B_body": "The cluster state updates from underlying clusters are now updated in batches, which should improve the cluster change propagation performance. Related to #12814.\n", "B_clean_body": ["cluster", "state", "updat", "underli", "cluster", "are", "now", "updat", "batch", "which", "improv", "cluster", "chang", "propag", "perform", "relat", "12814"], "title_sim": [0.8614011656203349], "body_sim": [0.5909170439434462], "file_list_sim": 0, "overlap_files_len": 0, "code_sim": [0.0, 0.0], "location_sim": [0.0, 0.0], "pattern": -1, "time": 69}, {"A_title": "Expand wildcards to closed indices in /_cat/indices", "A_clean_title": ["expand", "wildcard", "close", "indic", "cat", "indic"], "B_title": "Include closed indices as well for filter", "B_clean_title": ["includ", "close", "indic", "as", "well", "filter"], "A_body": "Closed indices are already displayed when no indices are explicitly selected. This commit ensures that closed indices are also shown when wildcard filtering is used (fixes #16419). It also addresses another issue that is caused by the fact that the cat action is based internally on 3 different cluster states (one when we query the cluster state to get all indices, one when we query cluster health, and one when we query indices stats). We currently fail the cat request when the user specifies a concrete index as parameter that does not exist. The implementation works as intended in that regard. It checks this not only for the first cluster state request, but also the subsequent indices stats one. This means that if the index is deleted before the cat action has queried the indices stats, it rightfully fails. In case the user provides wildcards (or no parameter at all), however, we fail the indices stats as we pass the resolved concrete indices to the indices stats request and fail to distinguish whether these indices have been resolved by wildcards or explicitly requested by the user. This means that if an index has been deleted before the indices stats request gets to execute, we fail the overall cat request (see #17395). The fix is to let the indices stats request do the resolving again and not pass the concrete indices.\n\nCloses #16419\nCloses #17395\n", "A_clean_body": ["close", "indic", "are", "alreadi", "display", "when", "no", "indic", "are", "explicitli", "select", "thi", "commit", "ensur", "that", "close", "indic", "are", "also", "shown", "when", "wildcard", "filter", "use", "fix", "16419", "it", "also", "address", "anoth", "issu", "that", "caus", "by", "fact", "that", "cat", "action", "base", "intern", "differ", "cluster", "state", "one", "when", "we", "queri", "cluster", "state", "get", "all", "indic", "one", "when", "we", "queri", "cluster", "health", "one", "when", "we", "queri", "indic", "stat", "we", "current", "fail", "cat", "request", "when", "user", "specifi", "concret", "index", "as", "paramet", "that", "not", "exist", "implement", "work", "as", "intend", "that", "regard", "it", "check", "thi", "not", "onli", "first", "cluster", "state", "request", "but", "also", "subsequ", "indic", "stat", "one", "thi", "mean", "that", "index", "delet", "befor", "cat", "action", "ha", "queri", "indic", "stat", "it", "right", "fail", "case", "user", "provid", "wildcard", "or", "no", "paramet", "at", "all", "howev", "we", "fail", "indic", "stat", "as", "we", "pass", "resolv", "concret", "indic", "indic", "stat", "request", "fail", "distinguish", "whether", "these", "indic", "have", "been", "resolv", "by", "wildcard", "or", "explicitli", "request", "by", "user", "thi", "mean", "that", "index", "ha", "been", "delet", "befor", "indic", "stat", "request", "get", "execut", "we", "fail", "overal", "cat", "request", "see", "17395", "fix", "let", "indic", "stat", "request", "resolv", "again", "not", "pass", "concret", "indic", "close", "16419", "close", "17395"], "B_body": "Fix for #16419\n", "B_clean_body": ["fix", "16419"], "title_sim": [0.4600347506268313], "body_sim": [0.055189251381886364], "file_list_sim": 0.5, "overlap_files_len": 1, "code_sim": [0.5005288956353616, 0.5005288956353616], "location_sim": [0.6075949367088608, 1.0], "pattern": 1, "time": 95}, {"A_title": "Forbid Collections.shuffle(List) in favor of Collections.shuffle(List,Random)", "A_clean_title": ["forbid", "collect", "shuffl", "list", "favor", "collect", "shuffl", "list", "random"], "B_title": "Remove and forbid use of Collections#shuffle(List) and Random#<init>()", "B_clean_title": ["remov", "forbid", "use", "collect", "shuffl", "list", "random", "init"], "A_body": "One can easily be fooled.\n", "A_clean_body": ["one", "easili", "fool"], "B_body": "This commit removes and now forbids all uses of\nCollections#shuffle(List) and Random#<init>() across the codebase. The\nrationale for removing and forbidding these methods is to increase test\nreproducibility. As these methods use non-reproducible seeds, production\ncode and tests that rely on these methods contribute to\nnon-reproducbility of tests.\n\nInstead of Collections#shuffle(List) the method\nCollections#shuffle(List, Random) can be used. All that is required then\nis a reproducible source of randomness. Consequently, the utility class\nRandomness has been added to assist in creating reproducible sources of\nrandomness.\n\nInstead of Random#<init>(), Random#<init>(long) with a reproducible seed\nor the aforementioned Randomess class can be used.\n\nCloses #15287\n", "B_clean_body": ["thi", "commit", "remov", "now", "forbid", "all", "use", "collect", "shuffl", "list", "random", "init", "across", "codebas", "rational", "remov", "forbid", "these", "method", "increas", "test", "reproduc", "as", "these", "method", "use", "non", "reproduc", "seed", "product", "code", "test", "that", "reli", "these", "method", "contribut", "non", "reproducbl", "test", "instead", "collect", "shuffl", "list", "method", "collect", "shuffl", "list", "random", "use", "all", "that", "requir", "then", "reproduc", "sourc", "random", "consequ", "util", "class", "random", "ha", "been", "ad", "assist", "creat", "reproduc", "sourc", "random", "instead", "random", "init", "random", "init", "long", "reproduc", "seed", "or", "aforement", "randomess", "class", "use", "close", "15287"], "title_sim": [0.7753764874702473], "body_sim": [0.022381233417597495], "file_list_sim": 0.0, "overlap_files_len": 0, "code_sim": [0.28884312444574695, 0.0], "location_sim": [0.0, 0.0], "pattern": 0, "time": 265}, {"A_title": "Update client.asciidoc", "A_clean_title": ["updat", "client", "asciidoc"], "B_title": "Qualify termQuery method under the Boolean Query section", "B_clean_title": ["qualifi", "termqueri", "term", "queri", "method", "under", "boolean", "queri", "section"], "A_body": "Correction of java code example in node client part.\n", "A_clean_body": ["correct", "java", "code", "exampl", "node", "client", "part"], "B_body": "As static keyword was not used when importing QueryBuilders, the termQuery method needs properly qualified when being passed to the must/mustNot methods in the Boolean Query section of the documentation\n", "B_clean_body": ["as", "static", "keyword", "wa", "not", "use", "when", "import", "querybuild", "queri", "builder", "termqueri", "term", "queri", "method", "need", "properli", "qualifi", "when", "be", "pass", "must", "mustnot", "must", "not", "method", "boolean", "queri", "section", "document"], "title_sim": [-0.055054080301052466], "body_sim": [0.06139734749078205], "file_list_sim": 0.0, "overlap_files_len": 0, "code_sim": [0.1435278982541368, 0.0], "location_sim": [0.0, 0.0], "pattern": 0, "time": 10}, {"A_title": "Special case the `_index` field in queries", "A_clean_title": ["special", "case", "index", "field", "queri"], "B_title": "Term(s) Query/Filter : add _index support", "B_clean_title": ["term", "queri", "filter", "add", "index", "support"], "A_body": "This change allows queries to test the name of the index by referring to the _index field in query clauses. Adding this capability means we can deprecate the specialist indices query.\n\nIndexFieldMapper is changed to make the term(s) query factory methods instead produce match_all or match_none queries based on tests of the index name.\n\nCloses #3316\n", "A_clean_body": ["thi", "chang", "allow", "queri", "test", "name", "index", "by", "refer", "index", "field", "queri", "claus", "ad", "thi", "capabl", "mean", "we", "deprec", "specialist", "indic", "queri", "indexfieldmapp", "index", "field", "mapper", "chang", "make", "term", "queri", "factori", "method", "instead", "produc", "match", "all", "or", "match", "none", "queri", "base", "test", "index", "name", "close", "3316"], "B_body": "This commit adds support for _index to the term query, terms query, term filter and terms filter.\nAll of the below queries work as expected :\n\n```\nGET /*/_search\n{\n  \"query\": {\n    \"term\": {\n    \"_index\": {\n      \"value\": \"reddit\"\n    }\n  }}\n}\n\nGET /*/_search\n{\n  \"query\" :\n  {\n    \"terms\": {\n      \"_index\": [\n        \"reddit2\"\n        ]\n    }\n  }\n}\n\nGET /*/_search\n{\n  \"query\":\n   {\n     \"filtered\": {\n       \"query\": {\n     \"match_all\": {}\n       },\n       \"filter\": {\n          \"terms\":\n          {\n            \"_index\": [  \"reddit\"]\n          }\n       }\n     }\n   }\n}\n\nGET /*/_search\n{\"query\":\n   {\n     \"filtered\": {\n       \"query\": {\n     \"match_all\": {}\n       },\n       \"filter\": {\n     \"term\": { \"_index\": { \"value\": \"reddit\" }}\n       }\n     }\n   } }\n```\n\nSince these are evaluated for each index (to resolve mappings) this change simply tests to see if the index is the correct one and returns a MATCH_ALL or MATCH_NO.\n\nCloses #3316\n", "B_clean_body": ["thi", "commit", "add", "support", "index", "term", "queri", "term", "queri", "term", "filter", "term", "filter", "all", "below", "queri", "work", "as", "expect", "get", "search", "queri", "term", "index", "valu", "reddit", "get", "search", "queri", "term", "index", "reddit2", "get", "search", "queri", "filter", "queri", "match", "all", "filter", "term", "index", "reddit", "get", "search", "queri", "filter", "queri", "match", "all", "filter", "term", "index", "valu", "reddit", "sinc", "these", "are", "evalu", "each", "index", "resolv", "map", "thi", "chang", "simpli", "test", "see", "index", "correct", "one", "return", "match", "all", "or", "match", "no", "close", "3316"], "title_sim": [0.28910665220784887], "body_sim": [0.6260446253419619], "file_list_sim": 0.0, "overlap_files_len": 0, "code_sim": [0.4196406129582531, 0.0], "location_sim": [0.0, 0.0], "pattern": 1, "time": 273}, {"A_title": "\"filter\" :  { ... },", "A_clean_title": ["filter"], "B_title": "fix mismatched curly bracket", "B_clean_title": ["fix", "mismatch", "curli", "bracket"], "A_body": "The ending braces.\n", "A_clean_body": ["end", "brace"], "B_body": "", "B_clean_body": [], "title_sim": [-0.042597026021049936], "body_sim": [0.0], "file_list_sim": 1.0, "overlap_files_len": 1, "code_sim": [0.0, 0.0], "location_sim": [1.0, 1.0], "pattern": 0, "time": 0}, {"A_title": "Add support for Lucene 5.4 GeoPoint queries", "A_clean_title": ["add", "support", "lucen", "geopoint", "geo", "point", "queri"], "B_title": "Added an `optimize_bbox` option for `geo_polygon` queries.", "B_clean_title": ["ad", "optim", "bbox", "option", "geo", "polygon", "queri"], "A_body": "This PR adds query support for Lucene 5.4 GeoPointField type along with backward compatibility for \"legacy\" geo_point indexes.\n", "A_clean_body": ["thi", "pr", "add", "queri", "support", "lucen", "geopointfield", "geo", "point", "field", "type", "along", "backward", "compat", "legaci", "geo", "point", "index"], "B_body": "This setting is analogous to the one for geo_distance queries, and defaults to \"memory\". Note\nthat the optimization will need to be updated when addressing #5968.\n\nCloses #10356.\n", "B_clean_body": ["thi", "set", "analog", "one", "geo", "distanc", "queri", "default", "memori", "note", "that", "optim", "will", "need", "updat", "when", "address", "5968", "close", "10356"], "title_sim": [0.4885833473613531], "body_sim": [0.3163853260091045], "file_list_sim": 0.08333333333333333, "overlap_files_len": 1, "code_sim": [0.5432262488185342, 0.05880489419292649], "location_sim": [0.0, 0.0], "pattern": -1, "time": 102}, {"A_title": "Remove support for new indexes using `path` setting in object/nested fields or `index_name` in any field", "A_clean_title": ["remov", "support", "new", "index", "path", "set", "object", "nest", "field", "or", "index", "name", "ani", "field"], "B_title": "Docs: Deprecated index_name in the docs", "B_clean_title": ["doc", "deprec", "index", "name", "doc"], "A_body": "Backcompat is still here for indexes created before 2.0. We can remove this backcompat when/if we implement mappings migration capabilities as @clintongormley described in #6677.\n\ncloses #6677\n\nFor reviewers: I did not conditionalize the writing of \"path\" because the value will always be the default as long as backcompat exists, and there is currently nothing written in the case of \"include_defaults\" (seems like an existing problem).  I can fix this if anyone feels strongly about it, but I opted to leave it alone since we are removing the setting anyways.\n", "A_clean_body": ["backcompat", "still", "here", "index", "creat", "befor", "we", "remov", "thi", "backcompat", "when", "we", "implement", "map", "migrat", "capabl", "as", "clintongormley", "describ", "6677", "close", "6677", "review", "did", "not", "condition", "write", "path", "becaus", "valu", "will", "alway", "default", "as", "long", "as", "backcompat", "exist", "there", "current", "noth", "written", "case", "includ", "default", "seem", "like", "exist", "problem", "fix", "thi", "anyon", "feel", "strongli", "about", "it", "but", "opt", "leav", "it", "alon", "sinc", "we", "are", "remov", "set", "anyway"], "B_body": "Relates to #6677\n", "B_clean_body": ["relat", "6677"], "title_sim": [0.2940173520686984], "body_sim": [0.027614795037143494], "file_list_sim": 0.08, "overlap_files_len": 2, "code_sim": [0.0, 0.0], "location_sim": [0.04835924006908463, 0.7368421052631579], "pattern": 1, "time": 210}, {"A_title": "Class permission for Groovy references", "A_clean_title": ["class", "permiss", "groovi", "refer"], "B_title": "Add permission to access groovy.lang.Reference from Groovy scripts", "B_clean_title": ["add", "permiss", "access", "groovi", "lang", "refer", "groovi", "script"], "A_body": "This commit adds a class permission for groovy.lang.Reference so they\ncan be used in scripts.\n\nCloses #16657\n", "A_clean_body": ["thi", "commit", "add", "class", "permiss", "groovi", "lang", "refer", "so", "they", "use", "script", "close", "16657"], "B_body": "Closes #16657\n", "B_clean_body": ["close", "16657"], "title_sim": [0.46365406259618586], "body_sim": [0.30517739918797165], "file_list_sim": 1.0, "overlap_files_len": 2, "code_sim": [0.8519372336827636, 0.8519372336827636], "location_sim": [0.5555555555555556, 0.5555555555555556], "pattern": 1, "time": 0}, {"A_title": "Add top_hits aggregation", "A_clean_title": ["add", "top", "hit", "aggreg"], "B_title": "Grouping prototype implementation", "B_clean_title": ["group", "prototyp", "implement"], "A_body": "The `top_hits` aggregator keeps track of the most relevant document being aggregated. This aggregator should be used as a sub aggregator of a bucket based aggregator, so that the top documents per bucket are computed.\n\nVia this aggregator [grouping / field collapsing](https://github.com/elasticsearch/elasticsearch/issues/256) can be achieved and is very versatile. Someone can group by a field (using a terms aggregator as parent) or by time (using a histogram aggregator as parent), in any case the parent bucket aggregator determines how to group. How correct the top hits will depend on the parent aggregator. For example when using the `terms` aggregator and the `top_hits` aggregator some document may not end up in the response, because the `shard_size` on the `terms` aggregator is less then the field's cardinality.\n\nThe `top_hits` aggregator should have the following options:\n- `size` - The amount of hits to collect.\n- `sort` - Defines how the top hits should be sorted.\n- and any other fetch phase options. Like source filtering and highlighting.\n\nThe prototype that is attached right now to this PR integrates nicely with the fetch phase, which allows all fetch like features to be implemented easily. Also it executes as if the `search_type` is set to `query_and_fetch`, this way aggregations don't need to execute extra round trips. \n\nExample usage of the current prototype:\n\n``` json\nGET /stack/question/_search?search_type=count\n{\n  \"aggs\": {\n    \"terms\": {\n      \"terms\": {\n        \"field\": \"tags\",\n        \"size\": 10\n      },\n      \"aggs\": {\n        \"top_tag_hits\": {\n          \"top_hits\": {\n            \"_source\": {\n              \"include\": [\n                \"title\"\n              ]\n            },\n            \"sort\": [\n              {\n                \"last_activity_date\": {\n                  \"order\": \"desc\"\n                }\n              }\n            ],\n            \"size\" : 3\n          }\n        }\n      }\n    }\n  }\n}\n```\n\nIn this example the hits are sorted by the field `last_activity_date` and only the top 3 hits are returned. Also per hit only the `title` field is included.\n\nResponse:\n\n``` json\n{\n   \"took\": 151,\n   \"timed_out\": false,\n   \"_shards\": {\n      \"total\": 5,\n      \"successful\": 5,\n      \"failed\": 0\n   },\n   \"hits\": {\n      \"total\": 175275,\n      \"max_score\": 0,\n      \"hits\": []\n   },\n   \"aggregations\": {\n      \"terms\": {\n         \"buckets\": [\n            {\n               \"key\": \"windows-7\",\n               \"doc_count\": 25365,\n               \"top_tag_hits\": {\n                  \"hits\": {\n                     \"total\": 25365,\n                     \"max_score\": 1,\n                     \"hits\": [\n                        {\n                           \"_index\": \"stack\",\n                           \"_type\": \"question\",\n                           \"_id\": \"602679\",\n                           \"_score\": 1,\n                           \"_source\": {\n                              \"title\": \"Windows port opening\"\n                           }\n                        },\n                        {\n                           \"_index\": \"stack\",\n                           \"_type\": \"question\",\n                           \"_id\": \"602570\",\n                           \"_score\": 1,\n                           \"_source\": {\n                              \"title\": \"Counter Strike Screen Resolution\"\n                           }\n                        },\n                        {\n                           \"_index\": \"stack\",\n                           \"_type\": \"question\",\n                           \"_id\": \"602249\",\n                           \"_score\": 1,\n                           \"_source\": {\n                              \"title\": \"Hardware error while burning DVD+\"\n                           }\n                        }\n                     ]\n                  }\n               }\n            },\n            {\n               \"key\": \"linux\",\n               \"doc_count\": 18342,\n               \"top_tag_hits\": {\n                  \"hits\": {\n                     \"total\": 18342,\n                     \"max_score\": 1,\n                     \"hits\": [\n                        {\n                           \"_index\": \"stack\",\n                           \"_type\": \"question\",\n                           \"_id\": \"602672\",\n                           \"_score\": 1,\n                           \"_source\": {\n                              \"title\": \"Ubuntu RFID Screensaver lock-unlock\"\n                           }\n                        },\n                        {\n                           \"_index\": \"stack\",\n                           \"_type\": \"question\",\n                           \"_id\": \"543625\",\n                           \"_score\": 1,\n                           \"_source\": {\n                              \"title\": \"Linux Mint doesn't boot after creating a swap partition\"\n                           }\n                        },\n                        {\n                           \"_index\": \"stack\",\n                           \"_type\": \"question\",\n                           \"_id\": \"602434\",\n                           \"_score\": 1,\n                           \"_source\": {\n                              \"title\": \"Is desktop pc support ssd and sata hard disk in one machine?\"\n                           }\n                        }\n                     ]\n                  }\n               }\n            },\n            {\n               \"key\": \"windows\",\n               \"doc_count\": 18119,\n               \"top_tag_hits\": {\n                  \"hits\": {\n                     \"total\": 18119,\n                     \"max_score\": 1,\n                     \"hits\": [\n                        {\n                           \"_index\": \"stack\",\n                           \"_type\": \"question\",\n                           \"_id\": \"602678\",\n                           \"_score\": 1,\n                           \"_source\": {\n                              \"title\": \"If I change my computers date / time, what could be affected?\"\n                           }\n                        },\n                        {\n                           \"_index\": \"stack\",\n                           \"_type\": \"question\",\n                           \"_id\": \"472446\",\n                           \"_score\": 1,\n                           \"_source\": {\n                              \"title\": \"Remove the Browser ballot app from Windows 8\"\n                           }\n                        },\n                        {\n                           \"_index\": \"stack\",\n                           \"_type\": \"question\",\n                           \"_id\": \"321988\",\n                           \"_score\": 1,\n                           \"_source\": {\n                              \"title\": \"How do I determine if my Windows is 32-bit or 64-bit using a command?\"\n                           }\n                        }\n                     ]\n                  }\n               }\n            },\n            {\n               \"key\": \"osx\",\n               \"doc_count\": 10971,\n               \"top_tag_hits\": {\n                  \"hits\": {\n                     \"total\": 10971,\n                     \"max_score\": 1,\n                     \"hits\": [\n                        {\n                           \"_index\": \"stack\",\n                           \"_type\": \"question\",\n                           \"_id\": \"602680\",\n                           \"_score\": 1,\n                           \"_source\": {\n                              \"title\": \"How to Install Google Chrome from the command line\"\n                           }\n                        },\n                        {\n                           \"_index\": \"stack\",\n                           \"_type\": \"question\",\n                           \"_id\": \"602482\",\n                           \"_score\": 1,\n                           \"_source\": {\n                              \"title\": \"All Mac OS X apps crash as opened\"\n                           }\n                        },\n                        {\n                           \"_index\": \"stack\",\n                           \"_type\": \"question\",\n                           \"_id\": \"517263\",\n                           \"_score\": 1,\n                           \"_source\": {\n                              \"title\": \"Create a shortcut for application on Google Chrome for MacOSX\"\n                           }\n                        }\n                     ]\n                  }\n               }\n            },\n            {\n               \"key\": \"ubuntu\",\n               \"doc_count\": 8743,\n               \"top_tag_hits\": {\n                  \"hits\": {\n                     \"total\": 8743,\n                     \"max_score\": 1,\n                     \"hits\": [\n                        {\n                           \"_index\": \"stack\",\n                           \"_type\": \"question\",\n                           \"_id\": \"602665\",\n                           \"_score\": 1,\n                           \"_source\": {\n                              \"title\": \"Add more partitions to Grub2 - Ubuntu\"\n                           }\n                        },\n                        {\n                           \"_index\": \"stack\",\n                           \"_type\": \"question\",\n                           \"_id\": \"190759\",\n                           \"_score\": 1,\n                           \"_source\": {\n                              \"title\": \"Ubuntu 10.04 Keyboard and Mouse Freezing Problem\"\n                           }\n                        },\n                        {\n                           \"_index\": \"stack\",\n                           \"_type\": \"question\",\n                           \"_id\": \"597297\",\n                           \"_score\": 1,\n                           \"_source\": {\n                              \"title\": \"Curly braces with LCtrl+LShift+LAlt+\u00e8 - how?\"\n                           }\n                        }\n                     ]\n                  }\n               }\n            },\n            {\n               \"key\": \"windows-xp\",\n               \"doc_count\": 7517,\n               \"top_tag_hits\": {\n                  \"hits\": {\n                     \"total\": 7517,\n                     \"max_score\": 1,\n                     \"hits\": [\n                        {\n                           \"_index\": \"stack\",\n                           \"_type\": \"question\",\n                           \"_id\": \"602679\",\n                           \"_score\": 1,\n                           \"_source\": {\n                              \"title\": \"Windows port opening\"\n                           }\n                        },\n                        {\n                           \"_index\": \"stack\",\n                           \"_type\": \"question\",\n                           \"_id\": \"510161\",\n                           \"_score\": 1,\n                           \"_source\": {\n                              \"title\": \"Windows 8 Hyper-V fails to boot Windows XP ISO\"\n                           }\n                        },\n                        {\n                           \"_index\": \"stack\",\n                           \"_type\": \"question\",\n                           \"_id\": \"180565\",\n                           \"_score\": 1,\n                           \"_source\": {\n                              \"title\": \"Logitech Optical Mouse Frozen In Middle of Windows XP Pro Screen\"\n                           }\n                        }\n                     ]\n                  }\n               }\n            },\n            {\n               \"key\": \"networking\",\n               \"doc_count\": 6739,\n               \"top_tag_hits\": {\n                  \"hits\": {\n                     \"total\": 6739,\n                     \"max_score\": 1,\n                     \"hits\": [\n                        {\n                           \"_index\": \"stack\",\n                           \"_type\": \"question\",\n                           \"_id\": \"602679\",\n                           \"_score\": 1,\n                           \"_source\": {\n                              \"title\": \"Windows port opening\"\n                           }\n                        },\n                        {\n                           \"_index\": \"stack\",\n                           \"_type\": \"question\",\n                           \"_id\": \"602645\",\n                           \"_score\": 1,\n                           \"_source\": {\n                              \"title\": \"are there any requirements for the sequence number on CP RST packets?\"\n                           }\n                        },\n                        {\n                           \"_index\": \"stack\",\n                           \"_type\": \"question\",\n                           \"_id\": \"602449\",\n                           \"_score\": 1,\n                           \"_source\": {\n                              \"title\": \"Vmware Dev Server not allowing HTTP traffic\"\n                           }\n                        }\n                     ]\n                  }\n               }\n            },\n            {\n               \"key\": \"mac\",\n               \"doc_count\": 5590,\n               \"top_tag_hits\": {\n                  \"hits\": {\n                     \"total\": 5590,\n                     \"max_score\": 1,\n                     \"hits\": [\n                        {\n                           \"_index\": \"stack\",\n                           \"_type\": \"question\",\n                           \"_id\": \"602482\",\n                           \"_score\": 1,\n                           \"_source\": {\n                              \"title\": \"All Mac OS X apps crash as opened\"\n                           }\n                        },\n                        {\n                           \"_index\": \"stack\",\n                           \"_type\": \"question\",\n                           \"_id\": \"602553\",\n                           \"_score\": 1,\n                           \"_source\": {\n                              \"title\": \"How can I load VLC instead of iTunes on my Mac when I press the player buttons on my Mac keyboard?\"\n                           }\n                        },\n                        {\n                           \"_index\": \"stack\",\n                           \"_type\": \"question\",\n                           \"_id\": \"602460\",\n                           \"_score\": 1,\n                           \"_source\": {\n                              \"title\": \"startup mac mini using boot usb with MASTER BOOT RECORD scheme but failed\"\n                           }\n                        }\n                     ]\n                  }\n               }\n            },\n            {\n               \"key\": \"wireless-networking\",\n               \"doc_count\": 4409,\n               \"top_tag_hits\": {\n                  \"hits\": {\n                     \"total\": 4409,\n                     \"max_score\": 1,\n                     \"hits\": [\n                        {\n                           \"_index\": \"stack\",\n                           \"_type\": \"question\",\n                           \"_id\": \"265142\",\n                           \"_score\": 1,\n                           \"_source\": {\n                              \"title\": \"Connect to Wi-Fi access point with specific MAC address\"\n                           }\n                        },\n                        {\n                           \"_index\": \"stack\",\n                           \"_type\": \"question\",\n                           \"_id\": \"602586\",\n                           \"_score\": 1,\n                           \"_source\": {\n                              \"title\": \"How to adjust Tx Power for Macbook Air mid-2012 Wi-Fi card\"\n                           }\n                        },\n                        {\n                           \"_index\": \"stack\",\n                           \"_type\": \"question\",\n                           \"_id\": \"435290\",\n                           \"_score\": 1,\n                           \"_source\": {\n                              \"title\": \"Use wifi and ethernet simultaneously?\"\n                           }\n                        }\n                     ]\n                  }\n               }\n            },\n            {\n               \"key\": \"windows-8\",\n               \"doc_count\": 3601,\n               \"top_tag_hits\": {\n                  \"hits\": {\n                     \"total\": 3601,\n                     \"max_score\": 1,\n                     \"hits\": [\n                        {\n                           \"_index\": \"stack\",\n                           \"_type\": \"question\",\n                           \"_id\": \"510161\",\n                           \"_score\": 1,\n                           \"_source\": {\n                              \"title\": \"Windows 8 Hyper-V fails to boot Windows XP ISO\"\n                           }\n                        },\n                        {\n                           \"_index\": \"stack\",\n                           \"_type\": \"question\",\n                           \"_id\": \"591388\",\n                           \"_score\": 1,\n                           \"_source\": {\n                              \"title\": \"Android USB Driver on Windows 8\"\n                           }\n                        },\n                        {\n                           \"_index\": \"stack\",\n                           \"_type\": \"question\",\n                           \"_id\": \"553339\",\n                           \"_score\": 1,\n                           \"_source\": {\n                              \"title\": \"Can I have 2 PDF documents open in Windows 8?\"\n                           }\n                        }\n                     ]\n                  }\n               }\n            }\n         ]\n      }\n   }\n}\n```\n", "A_clean_body": ["top", "hit", "aggreg", "keep", "track", "most", "relev", "document", "be", "aggreg", "thi", "aggreg", "use", "as", "sub", "aggreg", "bucket", "base", "aggreg", "so", "that", "top", "document", "per", "bucket", "are", "comput", "via", "thi", "aggreg", "group", "field", "collaps", "http", "github", "com", "elasticsearch", "elasticsearch", "issu", "256", "achiev", "veri", "versatil", "someon", "group", "by", "field", "term", "aggreg", "as", "parent", "or", "by", "time", "histogram", "aggreg", "as", "parent", "ani", "case", "parent", "bucket", "aggreg", "determin", "how", "group", "how", "correct", "top", "hit", "will", "depend", "parent", "aggreg", "exampl", "when", "term", "aggreg", "top", "hit", "aggreg", "some", "document", "may", "not", "end", "up", "respons", "becaus", "shard", "size", "term", "aggreg", "less", "then", "field", "'s", "cardin", "top", "hit", "aggreg", "have", "follow", "option", "size", "amount", "hit", "collect", "sort", "defin", "how", "top", "hit", "sort", "ani", "other", "fetch", "phase", "option", "like", "sourc", "filter", "highlight", "prototyp", "that", "attach", "right", "now", "thi", "pr", "integr", "nice", "fetch", "phase", "which", "allow", "all", "fetch", "like", "featur", "implement", "easili", "also", "it", "execut", "as", "search", "type", "set", "queri", "fetch", "thi", "way", "aggreg", "n't", "need", "execut", "extra", "round", "trip", "exampl", "usag", "current", "prototyp", "json", "get", "search", "stack", "question", "search", "type=count", "agg", "term", "term", "field", "tag", "size", "10", "agg", "top", "tag", "hit", "top", "hit", "sourc", "includ", "titl", "sort", "last", "activ", "date", "order", "desc", "size", "thi", "exampl", "hit", "are", "sort", "by", "field", "last", "activ", "date", "onli", "top", "hit", "are", "return", "also", "per", "hit", "onli", "titl", "field", "includ", "respons", "json", "took", "151", "time", "out", "fals", "shard", "total", "success", "fail", "hit", "total", "175275", "max", "score", "hit", "aggreg", "term", "bucket", "key", "window", "doc", "count", "25365", "top", "tag", "hit", "hit", "total", "25365", "max", "score", "hit", "index", "stack", "type", "question", "id", "602679", "score", "sourc", "titl", "window", "port", "open", "index", "stack", "type", "question", "id", "602570", "score", "sourc", "titl", "counter", "strike", "screen", "resolut", "index", "stack", "type", "question", "id", "602249", "score", "sourc", "titl", "hardwar", "error", "while", "burn", "dvd+", "key", "linux", "doc", "count", "18342", "top", "tag", "hit", "hit", "total", "18342", "max", "score", "hit", "index", "stack", "type", "question", "id", "602672", "score", "sourc", "titl", "ubuntu", "rfid", "screensav", "lock", "unlock", "index", "stack", "type", "question", "id", "543625", "score", "sourc", "titl", "linux", "mint", "n't", "boot", "after", "creat", "swap", "partit", "index", "stack", "type", "question", "id", "602434", "score", "sourc", "titl", "desktop", "pc", "support", "ssd", "sata", "hard", "disk", "one", "machin", "key", "window", "doc", "count", "18119", "top", "tag", "hit", "hit", "total", "18119", "max", "score", "hit", "index", "stack", "type", "question", "id", "602678", "score", "sourc", "titl", "chang", "my", "comput", "date", "time", "what", "could", "affect", "index", "stack", "type", "question", "id", "472446", "score", "sourc", "titl", "remov", "browser", "ballot", "app", "window", "index", "stack", "type", "question", "id", "321988", "score", "sourc", "titl", "how", "determin", "my", "window", "32", "bit", "or", "64", "bit", "command", "key", "osx", "doc", "count", "10971", "top", "tag", "hit", "hit", "total", "10971", "max", "score", "hit", "index", "stack", "type", "question", "id", "602680", "score", "sourc", "titl", "how", "instal", "googl", "chrome", "command", "line", "index", "stack", "type", "question", "id", "602482", "score", "sourc", "titl", "all", "mac", "os", "app", "crash", "as", "open", "index", "stack", "type", "question", "id", "517263", "score", "sourc", "titl", "creat", "shortcut", "applic", "googl", "chrome", "macosx", "mac", "osx", "key", "ubuntu", "doc", "count", "8743", "top", "tag", "hit", "hit", "total", "8743", "max", "score", "hit", "index", "stack", "type", "question", "id", "602665", "score", "sourc", "titl", "add", "more", "partit", "grub2", "ubuntu", "index", "stack", "type", "question", "id", "190759", "score", "sourc", "titl", "ubuntu", "10", "04", "keyboard", "mous", "freez", "problem", "index", "stack", "type", "question", "id", "597297", "score", "sourc", "titl", "curli", "brace", "lctrl+lshift+lalt+\u00e8", "ctrl+l", "shift+l", "alt+\u00e8", "how", "key", "window", "xp", "doc", "count", "7517", "top", "tag", "hit", "hit", "total", "7517", "max", "score", "hit", "index", "stack", "type", "question", "id", "602679", "score", "sourc", "titl", "window", "port", "open", "index", "stack", "type", "question", "id", "510161", "score", "sourc", "titl", "window", "hyper", "fail", "boot", "window", "xp", "iso", "index", "stack", "type", "question", "id", "180565", "score", "sourc", "titl", "logitech", "optic", "mous", "frozen", "middl", "window", "xp", "pro", "screen", "key", "network", "doc", "count", "6739", "top", "tag", "hit", "hit", "total", "6739", "max", "score", "hit", "index", "stack", "type", "question", "id", "602679", "score", "sourc", "titl", "window", "port", "open", "index", "stack", "type", "question", "id", "602645", "score", "sourc", "titl", "are", "there", "ani", "requir", "sequenc", "number", "cp", "rst", "packet", "index", "stack", "type", "question", "id", "602449", "score", "sourc", "titl", "vmware", "dev", "server", "not", "allow", "http", "traffic", "key", "mac", "doc", "count", "5590", "top", "tag", "hit", "hit", "total", "5590", "max", "score", "hit", "index", "stack", "type", "question", "id", "602482", "score", "sourc", "titl", "all", "mac", "os", "app", "crash", "as", "open", "index", "stack", "type", "question", "id", "602553", "score", "sourc", "titl", "how", "load", "vlc", "instead", "itun", "tune", "my", "mac", "when", "press", "player", "button", "my", "mac", "keyboard", "index", "stack", "type", "question", "id", "602460", "score", "sourc", "titl", "startup", "mac", "mini", "boot", "usb", "master", "boot", "record", "scheme", "but", "fail", "key", "wireless", "network", "doc", "count", "4409", "top", "tag", "hit", "hit", "total", "4409", "max", "score", "hit", "index", "stack", "type", "question", "id", "265142", "score", "sourc", "titl", "connect", "wi", "fi", "access", "point", "specif", "mac", "address", "index", "stack", "type", "question", "id", "602586", "score", "sourc", "titl", "how", "adjust", "tx", "power", "macbook", "air", "mid", "2012", "wi", "fi", "card", "index", "stack", "type", "question", "id", "435290", "score", "sourc", "titl", "use", "wifi", "ethernet", "simultan", "key", "window", "doc", "count", "3601", "top", "tag", "hit", "hit", "total", "3601", "max", "score", "hit", "index", "stack", "type", "question", "id", "510161", "score", "sourc", "titl", "window", "hyper", "fail", "boot", "window", "xp", "iso", "index", "stack", "type", "question", "id", "591388", "score", "sourc", "titl", "android", "usb", "driver", "window", "index", "stack", "type", "question", "id", "553339", "score", "sourc", "titl", "have", "pdf", "document", "open", "window"], "B_body": "Hi!\n\nI am trying to implement grouping feature and want to contribute to es. This is a proof-of-concept prototype to start conversation :), it is not meant to be pulled.\n\nWhat do i mean by grouping?\nLets say i have 4 Documents:\n{ \"id\" : 1, \"name\": \"Doc1\", \"field\" : \"A\" }\n{ \"id\" : 2, \"name\": \"Doc2\", \"field\" : \"A\" }\n{ \"id\" : 3, \"name\": \"Doc3\", \"field\" : \"B\" }\n{ \"id\" : 4, \"name\": \"Doc4\", \"field\" : \"B\" }\nI want to execute search that is something like this:\n{ \"query\": { \"match_all\" : {}, \"group\", \"field\" }\nWith result:\n{ \"field\" : {\n    \"A\" : { \"hits\" : { \"id\" : 1, \"_source\" : { \"id\" : 1, \"name\": \"Doc1\", \"field\" : \"A\" } },\n                           { \"id\" : 2, \"_source\" : { \"id\" : 2, \"name\": \"Doc2\", \"field\" : \"A\" } }\n             },\n    \"B\" : { \"hits\" : { \"id\" : 3, \"_source\" : { \"id\" : 3, \"name\": \"Doc3\", \"field\" : \"B\" } },\n                           { \"id\" : 4, \"_source\" : { \"id\" : 4, \"name\": \"Doc4\", \"field\" : \"B\" } }\n            }\n}\n\nImplementation.\nFirst i looked at lucene-grouping module, but later decided to make a simple (slower?) one-pass solution based on Facets. Basically, in aggregator i collect docs in maps, and later - sort them and merge into one map.\n\nThe code is a work-in-progress for now, because i was unable to fit nicely it into current hierarchy (SearchPhaseController operates only with flat ScoreDocs array, or i have to reimplement whole Search stack from sratch).\nAny thought?\n\nThank!\n", "B_clean_body": ["hi", "am", "tri", "implement", "group", "featur", "want", "contribut", "es", "thi", "proof", "concept", "prototyp", "start", "convers", "it", "not", "meant", "pull", "what", "mean", "by", "group", "let", "say", "have", "document", "id", "name", "doc1", "field", "id", "name", "doc2", "field", "id", "name", "doc3", "field", "id", "name", "doc4", "field", "want", "execut", "search", "that", "someth", "like", "thi", "queri", "match", "all", "group", "field", "result", "field", "hit", "id", "sourc", "id", "name", "doc1", "field", "id", "sourc", "id", "name", "doc2", "field", "hit", "id", "sourc", "id", "name", "doc3", "field", "id", "sourc", "id", "name", "doc4", "field", "implement", "first", "look", "at", "lucen", "group", "modul", "but", "later", "decid", "make", "simpl", "slower", "one", "pass", "solut", "base", "facet", "basic", "aggreg", "collect", "doc", "map", "later", "sort", "them", "merg", "into", "one", "map", "code", "work", "progress", "now", "becaus", "wa", "unabl", "fit", "nice", "it", "into", "current", "hierarchi", "searchphasecontrol", "search", "phase", "control", "oper", "onli", "flat", "scoredoc", "score", "doc", "array", "or", "have", "reimplement", "whole", "search", "stack", "sratch", "ani", "thought", "thank"], "title_sim": [0.01024009948411055], "body_sim": [0.4284126421405294], "file_list_sim": 0, "overlap_files_len": 0, "code_sim": [0.0, 0.0], "location_sim": [0.0, 0.0], "pattern": 0, "time": 574}, {"A_title": "Add PathHierarchy type back to path_hierarchy tokenizer for backward compatibility with 1.x", "A_clean_title": ["add", "pathhierarchi", "path", "hierarchi", "type", "back", "path", "hierarchi", "token", "backward", "compat"], "B_title": "Update pathhierarchy-tokenizer.asciidoc", "B_clean_title": ["updat", "pathhierarchi", "token", "asciidoc"], "A_body": "Relates to #15756\n", "A_clean_body": ["relat", "15756"], "B_body": "The docs seem to contain a bug regarding the type of the path hierarchy tokenizer.\n", "B_clean_body": ["doc", "seem", "contain", "bug", "regard", "type", "path", "hierarchi", "token"], "title_sim": [0.10941423634777943], "body_sim": [0.04805985975444433], "file_list_sim": 0.0, "overlap_files_len": 0, "code_sim": [0.0, 0.0], "location_sim": [0.0, 0.0], "pattern": 0, "time": 439}, {"A_title": "Build and attach Javadoc artifact on release", "A_clean_title": ["build", "attach", "javadoc", "artifact", "releas"], "B_title": "Add javadocs jars", "B_clean_title": ["add", "javadoc", "jar"], "A_body": "This PR creates and attaches the Javadoc artifact for Elasticsearch (core) during a release and will upload that artifact to the Maven repository, along with the original binary artifact and the source artifact.\n\nThis change would enable users and developers use a service like http://www.javadoc.io/ to read the Elasticsearch Javadoc instead of having to build it themselves or read the complete source code.\n\nIf this PR is being merged (or at least a candidate for merging), I would port it to the Gradle build in the `master` branch as well.\n\nRefs #1203\n", "A_clean_body": ["thi", "pr", "creat", "attach", "javadoc", "artifact", "elasticsearch", "core", "dure", "releas", "will", "upload", "that", "artifact", "maven", "repositori", "along", "origin", "binari", "artifact", "sourc", "artifact", "thi", "chang", "would", "enabl", "user", "develop", "use", "servic", "like", "http", "javadoc", "www", "io", "read", "elasticsearch", "javadoc", "instead", "have", "build", "it", "themselv", "or", "read", "complet", "sourc", "code", "thi", "pr", "be", "merg", "or", "at", "least", "candid", "merg", "would", "port", "it", "gradl", "build", "master", "branch", "as", "well", "ref", "1203"], "B_body": "This change adds javadoc jars to core, test-framework and plugins. There\nwere a couple issues which javadoc found, but doclint did not already\nfind.\n", "B_clean_body": ["thi", "chang", "add", "javadoc", "jar", "core", "test", "framework", "plugin", "there", "were", "coupl", "issu", "which", "javadoc", "found", "but", "doclint", "did", "not", "alreadi", "find"], "title_sim": [0.026085785644805896], "body_sim": [0.16886093341192315], "file_list_sim": 0, "overlap_files_len": 0, "code_sim": [0.0, 0.0], "location_sim": [0.0, 0.0], "pattern": 0, "time": 8}, {"A_title": "Remove sleep based testing of http pipelining", "A_clean_title": ["remov", "sleep", "base", "test", "http", "pipelin"], "B_title": "Address race condition in HTTP pipeline tests", "B_clean_title": ["address", "race", "condit", "http", "pipelin", "test"], "A_body": "Currently the http pipelining tests check the order of responses with\npieplining enabled and disabled using sleeps in the requests to\nrandomize the order the requests write their response. This change\nremoves the sleeps, and instead makes the order the threads return in\ndeterministic using latches, where each thread waits on the latch from\nthe previous thread.\n\nNote that the test isn't really any faster (the multi threaded nature\nstill takes take for everything to converge), but at least there is no\nlonger any way for the possibility of a race condition.\n", "A_clean_body": ["current", "http", "pipelin", "test", "check", "order", "respons", "pieplin", "enabl", "disabl", "sleep", "request", "random", "order", "request", "write", "their", "respons", "thi", "chang", "remov", "sleep", "instead", "make", "order", "thread", "return", "determinist", "latch", "where", "each", "thread", "wait", "latch", "previou", "thread", "note", "that", "test", "n't", "realli", "ani", "faster", "multi", "thread", "natur", "still", "take", "take", "everyth", "converg", "but", "at", "least", "there", "no", "longer", "ani", "way", "possibl", "race", "condit"], "B_body": "The Netty 4 HTTP server pipeline tests contains two different test\ncases. The general idea behind these tests is to submit some requests to\na Netty 4 HTTP server, one test with pipelining enabled and another test\nwith pipelining disabled. These requests are submitted to two endpoints,\none with a path like /{id} and another with a path like /slow with a\nquery string parameter sleep. This parameter tells the request handler\nhow long to sleep for before replying. The idea is that in the case of\nthe pipelining enabled tests, the requests should come back exactly in\nthe order submitted, even with some of the requests hitting the slow\nendpoint with random sleep durations; this is the guarantee that\npipelining provides. And in the case of the pipelining disabled tests,\nrequests were randombly submitted to /{id} and /slow with sleep\nparameters starting at 600ms and increasing by 100ms for each slow\nrequest constructed. We would expect the requests to come back with the\nall the responses to the /{id} requests first because these requests\nwill execute instantaneously, and then the responses to the /slow\nrequests. Further, it was expected that the slow requests would come\nback ordered by the length of the sleep, the thinking being that 100ms\nshould be enough of a difference between each request that we would\navoid any race conditions. Sadly, this is not the case, the threads do\nsometimes hit race conditions.\n\nThis commit modifies the HTTP server pipelining tests to address this\nrace condition. The modification is that the query string parameter on\nthe /slow endpoint is removed in favor of just submitting requests to\nthe path /slow/{id}, where id just used a marker to distinguish each\nrequest. The server chooses a random sleep of at least 500ms for each\nrequest on the slow path. The assertion here then is that the /{id}\nresponses arrive first, then then /slow responses. We can not make an\nassertion on the order of the responses, but we can assert that we did\nsee every expected response.\n", "B_clean_body": ["netti", "http", "server", "pipelin", "test", "contain", "two", "differ", "test", "case", "gener", "idea", "behind", "these", "test", "submit", "some", "request", "netti", "http", "server", "one", "test", "pipelin", "enabl", "anoth", "test", "pipelin", "disabl", "these", "request", "are", "submit", "two", "endpoint", "one", "path", "like", "id", "anoth", "path", "like", "slow", "queri", "string", "paramet", "sleep", "thi", "paramet", "tell", "request", "handler", "how", "long", "sleep", "befor", "repli", "idea", "that", "case", "pipelin", "enabl", "test", "request", "come", "back", "exactli", "order", "submit", "even", "some", "request", "hit", "slow", "endpoint", "random", "sleep", "durat", "thi", "guarante", "that", "pipelin", "provid", "case", "pipelin", "disabl", "test", "request", "were", "randombl", "submit", "id", "slow", "sleep", "paramet", "start", "at", "600m", "increas", "by", "100m", "each", "slow", "request", "construct", "we", "would", "expect", "request", "come", "back", "all", "respons", "id", "request", "first", "becaus", "these", "request", "will", "execut", "instantan", "then", "respons", "slow", "request", "further", "it", "wa", "expect", "that", "slow", "request", "would", "come", "back", "order", "by", "length", "sleep", "think", "be", "that", "100m", "enough", "differ", "between", "each", "request", "that", "we", "would", "avoid", "ani", "race", "condit", "sadli", "thi", "not", "case", "thread", "sometim", "hit", "race", "condit", "thi", "commit", "modifi", "http", "server", "pipelin", "test", "address", "thi", "race", "condit", "modif", "that", "queri", "string", "paramet", "slow", "endpoint", "remov", "favor", "just", "submit", "request", "path", "slow", "id", "where", "id", "just", "use", "marker", "distinguish", "each", "request", "server", "choos", "random", "sleep", "at", "least", "500m", "each", "request", "slow", "path", "assert", "here", "then", "that", "id", "respons", "arriv", "first", "then", "then", "slow", "respons", "we", "not", "make", "assert", "order", "respons", "but", "we", "assert", "that", "we", "did", "see", "everi", "expect", "respons"], "title_sim": [0.7052687581410024], "body_sim": [0.41803450059568326], "file_list_sim": 1.0, "overlap_files_len": 1, "code_sim": [0.001864367499921738, 0.001864367499921738], "location_sim": [0.6899563318777293, 0.6899563318777293], "pattern": 0, "time": 2}, {"A_title": "Introducing VersionType.FORCE & VersionType.EXTERNAL_GTE", "A_clean_title": ["introduc", "versiontyp", "forc", "version", "type", "versiontyp", "version", "type", "extern", "gte"], "B_title": "implement external_relaxed versioning type - (Doesn't throw VersionConfl...", "B_clean_title": ["implement", "extern", "relax", "version", "type", "n't", "throw", "versionconfl", "version", "confl"], "A_body": "Also added \"external_gt\" as an alias name for VersionType.EXTERNAL , accessible for the rest layer.\n\nCloses #4213 , Closes #2946\n", "A_clean_body": ["also", "ad", "extern", "gt", "as", "alia", "name", "versiontyp", "extern", "version", "type", "access", "rest", "layer", "close", "4213", "close", "2946"], "B_body": "...ictExceptions)\n\nI wanted to change elasticsearch so it didn't throw a VersionConflictException if it received a create/index/delete request for a version less than the current index version. It returns the normal 'ok' response but the version number returned is the latest index version.\n\nThe reasoning behind this is; I am writing a very thin wrapper to elasticsearch, and any request into the wrapper after elasticsearch is put onto a messagequeue, if putting on that queue fails I return a specific 'temporary' error and the client is supposed to resend the request. When they resend the request, I didn't want it to fail at elasticsearch, hence this version type of external_relaxed....\n\nfeedback appreciated.\n", "B_clean_body": ["ictexcept", "ict", "except", "want", "chang", "elasticsearch", "so", "it", "did", "n't", "throw", "versionconflictexcept", "version", "conflict", "except", "it", "receiv", "creat", "index", "delet", "request", "version", "less", "than", "current", "index", "version", "it", "return", "normal", "'ok", "respons", "but", "version", "number", "return", "latest", "index", "version", "reason", "behind", "thi", "am", "write", "veri", "thin", "wrapper", "elasticsearch", "ani", "request", "into", "wrapper", "after", "elasticsearch", "put", "onto", "messagequeu", "put", "that", "queue", "fail", "return", "specif", "'temporari", "error", "client", "suppos", "resend", "request", "when", "they", "resend", "request", "did", "n't", "want", "it", "fail", "at", "elasticsearch", "henc", "thi", "version", "type", "extern", "relax", "feedback", "appreci"], "title_sim": [0.6888940069242205], "body_sim": [0.24145710908345258], "file_list_sim": 0.0, "overlap_files_len": 0, "code_sim": [0.6340651907255863, 0.0], "location_sim": [0.0, 0.0], "pattern": 0, "time": 811}, {"A_title": "Change environment variable for terrible JVM workaround", "A_clean_title": ["chang", "environ", "variabl", "terribl", "jvm", "workaround"], "B_title": "Fix typo in JVM checker user help.", "B_clean_title": ["fix", "typo", "jvm", "checker", "user", "help"], "A_body": "If the JVM check fails, the \"if you absolutely cannot upgrade\" message mentions adding a workaround flag to the JVM_OPTS environment variable. Shouldn't this be the JAVA_OPTS environment variable instead? I had to modify JAVA_OPTS to get the workaround to work (at least when launching through elasticsearch.bat on Windows).\n", "A_clean_body": ["jvm", "check", "fail", "you", "absolut", "not", "upgrad", "messag", "mention", "ad", "workaround", "flag", "jvm", "opt", "environ", "variabl", "n't", "thi", "java", "opt", "environ", "variabl", "instead", "had", "modifi", "java", "opt", "get", "workaround", "work", "at", "least", "when", "launch", "through", "elasticsearch", "bat", "window"], "B_body": "When checking the JVM currently running ES we provide the user with\nhelp on which environment variable to use to disable the check in\ncase the check fails. The variable we point to however is the wrong\none.\n\n(As discussed by mail previously.)\n", "B_clean_body": ["when", "check", "jvm", "current", "run", "es", "we", "provid", "user", "help", "which", "environ", "variabl", "use", "disabl", "check", "case", "check", "fail", "variabl", "we", "point", "howev", "wrong", "one", "as", "discuss", "by", "mail", "previous"], "title_sim": [0.350397815289494], "body_sim": [0.3935856506514292], "file_list_sim": 1.0, "overlap_files_len": 1, "code_sim": [1.0, 1.0], "location_sim": [1.0, 1.0], "pattern": 0, "time": 7}, {"A_title": "Aggregations: Significant Terms Heuristics now registered correctly", "A_clean_title": ["aggreg", "signific", "term", "heurist", "now", "regist", "correctli"], "B_title": "Added missing module registration in TransportClient for Significant Terms", "B_clean_title": ["ad", "miss", "modul", "registr", "transportcli", "transport", "client", "signific", "term"], "A_body": "Closes #7840\n", "A_clean_body": ["close", "7840"], "B_body": "Required for serialising significant_terms agg responses\nCloses #7840\n", "B_clean_body": ["requir", "serialis", "signific", "term", "agg", "respons", "close", "7840"], "title_sim": [0.3139595006448195], "body_sim": [0.28729748764629176], "file_list_sim": 0.0, "overlap_files_len": 0, "code_sim": [0.5963638815343594, 0.0], "location_sim": [0.0, 0.0], "pattern": 1, "time": 0}, {"A_title": "Enforce supported Maven versions", "A_clean_title": ["enforc", "support", "maven", "version"], "B_title": "[build] enforce maven version for rpm module", "B_clean_title": ["build", "enforc", "maven", "version", "rpm", "modul"], "A_body": "Our builds are currently not compatible with Maven versions `< 3.1.0` and\n`>= 3.3.0`. This commit will enforce Maven 3 versions that our builds are\nknown to be compatible with. We will consider these our supported Maven\nversions.\n\nCloses #13022\n", "A_clean_body": ["our", "build", "are", "current", "not", "compat", "maven", "version", "thi", "commit", "will", "enforc", "maven", "version", "that", "our", "build", "are", "known", "compat", "we", "will", "consid", "these", "our", "support", "maven", "version", "close", "13022"], "B_body": "When RPM module is supposed to be built, we need to have at least maven 3.2.3 to build the package.\n\nOtherwise the finalName we get is `elasticsearch-2.0.0-beta1_SNAPSHOT20150820131523.noarch.rpm` instead of `elasticsearch-2.0.0-beta1-SNAPSHOT.rpm`.\n", "B_clean_body": ["when", "rpm", "modul", "suppos", "built", "we", "need", "have", "at", "least", "maven", "build", "packag", "otherwis", "finalnam", "final", "name", "we", "get", "elasticsearch", "noarch", "rpm", "beta1", "snapshot20150820131523", "instead", "elasticsearch", "beta1", "snapshot", "rpm"], "title_sim": [0.6548176746446731], "body_sim": [0.2307785203388455], "file_list_sim": 0.0, "overlap_files_len": 0, "code_sim": [0.8516704866073593, 0.0], "location_sim": [0.0, 0.0], "pattern": -1, "time": 0}, {"A_title": "Fix `highlight_query` in percolator", "A_clean_title": ["fix", "highlight", "queri", "percol"], "B_title": "Make highlight query also work in the percolate api", "B_clean_title": ["make", "highlight", "queri", "also", "work", "percol", "api"], "A_body": "When highlighting with the percolator a `highlight_query` caused UnsupportedOperationException\n", "A_clean_body": ["when", "highlight", "percol", "highlight", "queri", "caus", "unsupportedoperationexcept", "unsupport", "oper", "except"], "B_body": "Percolate is throwing unsupported operation exception when highlight query is being used. This PR fixes that.\n", "B_clean_body": ["percol", "throw", "unsupport", "oper", "except", "when", "highlight", "queri", "be", "use", "thi", "pr", "fix", "that"], "title_sim": [0.735526719490512], "body_sim": [0.7590824104315665], "file_list_sim": 0.2, "overlap_files_len": 1, "code_sim": [0.23127773838640384, 0.6155181960953771], "location_sim": [0.13615023474178403, 0.42028985507246375], "pattern": 0, "time": 0}, {"A_title": "Improve upgrade docs", "A_clean_title": ["improv", "upgrad", "doc"], "B_title": "upgrading to 1.0 docs", "B_clean_title": ["upgrad", "doc"], "A_body": "Added upgrade.asciidoc and supporting links to README.textfile and setup.asciidoc.\n", "A_clean_body": ["ad", "upgrad", "asciidoc", "support", "link", "readm", "textfil", "setup", "asciidoc"], "B_body": "Thought it might be helpful to mention the full cluster restart in a more prominent location. \n", "B_clean_body": ["thought", "it", "might", "help", "mention", "full", "cluster", "restart", "more", "promin", "locat"], "title_sim": [0.7819021396916443], "body_sim": [0.006015889854284493], "file_list_sim": 0, "overlap_files_len": 0, "code_sim": [0.0, 0.0], "location_sim": [0.0, 0.0], "pattern": 0, "time": 39}, {"A_title": "Remove PROTO-based custom cluster state components", "A_clean_title": ["remov", "proto", "base", "custom", "cluster", "state", "compon"], "B_title": " Better extension point for custom cluster state parts", "B_clean_title": ["better", "extens", "point", "custom", "cluster", "state", "part"], "A_body": "Switches custom cluster state components from PROTO-based de-serialization to named objects based de-serialization\r\n\r\nCloses #21868 \r\n\r\nThis PR removes PROTO-based serialization from all custom objects except custom IndexMetaData. Custom IndexMetaData is not currently used in any know code and will be removed in v7.0. I will open a separate PR deprecating it and disabling creation of Custom IndexMetaData for new indices in v6.0.", "A_clean_body": ["switch", "custom", "cluster", "state", "compon", "proto", "base", "de", "serial", "name", "object", "base", "de", "serial", "close", "21868", "thi", "pr", "remov", "proto", "base", "serial", "all", "custom", "object", "except", "custom", "indexmetadata", "index", "meta", "data", "custom", "indexmetadata", "index", "meta", "data", "not", "current", "use", "ani", "know", "code", "will", "remov", "v7", "will", "open", "separ", "pr", "deprec", "it", "disabl", "creation", "custom", "indexmetadata", "index", "meta", "data", "new", "indic", "v6"], "B_body": "This adds a better extension point for custom cluster state parts by only allowing adding custom cluster state parts via the ClusterPlugin interface.\r\n\r\nThis is a WIP and would be great to get feedback.\r\n\r\n* By piggybacking on the named writeable infrastructure we can serialize custom cluster state parts for the internal protocol. This seems to workout nice as the serialization code doesn't need to know about custom cluster metadata implementations.\r\n* For the xcontent serialization there is no such thing as named writaebles, so I passed down a registry holding prototypes for the custom cluster state parts on all the places we need to do de-serialization. We can maybe consider adding something like named writable on top of XContentParser? However passing down the registry doesn't seem to be too bad.\r\n* Should custom index metadata be removed? It isn't used in any place. I think it should, but in a follow up change.\r\n* This change needs tests.\r\n\r\nPR for #20888", "B_clean_body": ["thi", "add", "better", "extens", "point", "custom", "cluster", "state", "part", "by", "onli", "allow", "ad", "custom", "cluster", "state", "part", "via", "clusterplugin", "cluster", "plugin", "interfac", "thi", "wip", "would", "great", "get", "feedback", "by", "piggyback", "name", "writeabl", "infrastructur", "we", "serial", "custom", "cluster", "state", "part", "intern", "protocol", "thi", "seem", "workout", "nice", "as", "serial", "code", "n't", "need", "know", "about", "custom", "cluster", "metadata", "implement", "xcontent", "serial", "there", "no", "such", "thing", "as", "name", "writaebl", "so", "pass", "down", "registri", "hold", "prototyp", "custom", "cluster", "state", "part", "all", "place", "we", "need", "de", "serial", "we", "mayb", "consid", "ad", "someth", "like", "name", "writabl", "top", "xcontentpars", "content", "parser", "howev", "pass", "down", "registri", "n't", "seem", "too", "bad", "custom", "index", "metadata", "remov", "it", "n't", "use", "ani", "place", "think", "it", "but", "follow", "up", "chang", "thi", "chang", "need", "test", "pr", "20888"], "title_sim": [0.5125389721004936], "body_sim": [0.5587353748992722], "file_list_sim": 0, "overlap_files_len": 0, "code_sim": [0.0, 0.0], "location_sim": [0.0, 0.0], "pattern": -1, "time": 51}, {"A_title": "Aggregations: Fixed DateHistogramBuilder to accept preOffset and postOff...", "A_clean_title": ["aggreg", "fix", "datehistogrambuild", "date", "histogram", "builder", "accept", "preoffset", "pre", "offset", "postoff", "post", "off"], "B_title": "Fix offsets in DateHistogramBuilder", "B_clean_title": ["fix", "offset", "datehistogrambuild", "date", "histogram", "builder"], "A_body": "...set as Strings\n\nThis is what DateHistogramParser expects so will enable the builder to build valid requests using these variables.\nAlso added tests for preOffset and postOffset since these tests did not exist\n\nCloses #5586\n", "A_clean_body": ["set", "as", "string", "thi", "what", "datehistogrampars", "date", "histogram", "parser", "expect", "so", "will", "enabl", "builder", "build", "valid", "request", "these", "variabl", "also", "ad", "test", "preoffset", "pre", "offset", "postoffset", "post", "offset", "sinc", "these", "test", "did", "not", "exist", "close", "5586"], "B_body": "This PR fixes #5586 by correcting the behavior of the current `preOffset` and `postOffset` methods in DateHistogramBuilder and adds new versions of these methods that take `DateHistogram.Interval`.  It is patterned after the interval property that already exists in this class.\n\nThis PR is an alternative to #5587.  That PR removes the current methods and replaces them with `String` based fields.  This seems inconsistent with the other methods in the class and causes a change to the contract currently established by DateHistogramBuilder.\n", "B_clean_body": ["thi", "pr", "fix", "5586", "by", "correct", "behavior", "current", "preoffset", "pre", "offset", "postoffset", "post", "offset", "method", "datehistogrambuild", "date", "histogram", "builder", "add", "new", "version", "these", "method", "that", "take", "datehistogram", "interv", "date", "histogram", "it", "pattern", "after", "interv", "properti", "that", "alreadi", "exist", "thi", "class", "thi", "pr", "altern", "5587", "that", "pr", "remov", "current", "method", "replac", "them", "string", "base", "field", "thi", "seem", "inconsist", "other", "method", "class", "caus", "chang", "contract", "current", "establish", "by", "datehistogrambuild", "date", "histogram", "builder"], "title_sim": [0.7291340586737571], "body_sim": [0.41523456503228756], "file_list_sim": 1.0, "overlap_files_len": 2, "code_sim": [0.7123212475948141, 0.7123212475948141], "location_sim": [0.27169811320754716, 0.27169811320754716], "pattern": 1, "time": 53}, {"A_title": "Add date math support in index names", "A_clean_title": ["add", "date", "math", "support", "index", "name"], "B_title": "Add '?' (one character) matching in index name wildcard patterns", "B_clean_title": ["add", "one", "charact", "match", "index", "name", "wildcard", "pattern"], "A_body": "Date math index name resolution enables you to search a range of time-series indices, rather than searching all of your time-series indices and filtering the the results or maintaining aliases. Limiting the number of indices that are searched reduces the load on the cluster and improves execution performance. For example, if you are searching for errors in your daily logs, you can use a date math name template to restrict the search to the past two days.\n\nThe added `ExpressionResolver` implementation that is responsible for resolving date math expressions in index names. This resolver is evaluated before wildcard expressions are evaluated.\n\nThe supported format: `<static_name{date_math_expr{date_format|timezone_id}}>` and the date math expressions must be enclosed within angle brackets. The `date_format` is optional and defaults to `YYYY.MM.dd`. The `timezone_id` id is optional too and defaults to `utc`.\n\nThe `{` character can be escaped by places `\\\\` before it.\n\nThe following index names can be specified: (assuming now is 2024.03.22 noon utc)\n- `<logstash-{now/d}>` translates into `logstash-2024.03.22`\n- `<logstash-{now/M}>` translates into `logstash-2024.03.01`\n- `<logstash-{now/M{YYYY.MM}}>` translates into `logstash-2024.03`\n- `<logstash-{now/M-1M{YYYY.MM}}>` translates into `logstash-2024.02`\n- `<logstash-{now/d{YYYY.MM.dd|+12:00}}>` translates into `logstash-2024.03.23`\n\nThe following example shows a search request that searches the Logstash indices for the past\nthree days:\n\n``` bash\ncurl -XGET 'localhost:9200/<logstash-{now/d-2d}>,<logstash-{now/d-1d}>,<logstash-{now/d}>/_search' {\n  \"query\" : {\n    ...\n  }\n}\n```\n\nPR for #12059\n", "A_clean_body": ["date", "math", "index", "name", "resolut", "enabl", "you", "search", "rang", "time", "seri", "indic", "rather", "than", "search", "all", "your", "time", "seri", "indic", "filter", "result", "or", "maintain", "alias", "limit", "number", "indic", "that", "are", "search", "reduc", "load", "cluster", "improv", "execut", "perform", "exampl", "you", "are", "search", "error", "your", "daili", "log", "you", "use", "date", "math", "name", "templat", "restrict", "search", "past", "two", "day", "ad", "expressionresolv", "express", "resolv", "implement", "that", "respons", "resolv", "date", "math", "express", "index", "name", "thi", "resolv", "evalu", "befor", "wildcard", "express", "are", "evalu", "support", "format", "static", "name", "date", "math", "expr", "date", "format|timezon", "id", "date", "math", "express", "must", "enclos", "within", "angl", "bracket", "date", "format", "option", "default", "yyyi", "mm", "dd", "timezon", "id", "id", "option", "too", "default", "utc", "charact", "escap", "by", "place", "befor", "it", "follow", "index", "name", "specifi", "assum", "now", "2024", "03", "22", "noon", "utc", "logstash", "now", "translat", "into", "logstash", "2024", "03", "22", "logstash", "now", "translat", "into", "logstash", "2024", "03", "01", "logstash", "now", "yyyi", "mm", "translat", "into", "logstash", "2024", "03", "logstash", "1m", "now", "yyyi", "mm", "translat", "into", "logstash", "2024", "02", "logstash", "now", "yyyi", "mm", "dd|+12:00", "translat", "into", "logstash", "2024", "03", "23", "follow", "exampl", "show", "search", "request", "that", "search", "logstash", "indic", "past", "three", "day", "bash", "curl", "xget", "'localhost:9200", "logstash", "2d", "now", "logstash", "1d", "now", "logstash", "now", "search", "queri", "pr", "12059"], "B_body": "Expands wildcard comparison patterns with `?`, that represents exactly one character. Uses Pattern and Matcher classes instead of previous custom implementation. Closes #8824\nDoes this feature make sense? I used Pattern and Matcher classes for this, with whitelisted `.` and `.*` sequences. I also have a custom matching implementation, but currently it doesn't work for some rare cases.\n", "B_clean_body": ["expand", "wildcard", "comparison", "pattern", "that", "repres", "exactli", "one", "charact", "use", "pattern", "matcher", "class", "instead", "previou", "custom", "implement", "close", "8824", "thi", "featur", "make", "sens", "use", "pattern", "matcher", "class", "thi", "whitelist", "sequenc", "also", "have", "custom", "match", "implement", "but", "current", "it", "n't", "work", "some", "rare", "case"], "title_sim": [0.4185644132334211], "body_sim": [0.06765523948663286], "file_list_sim": 0.058823529411764705, "overlap_files_len": 1, "code_sim": [0.07629131173136296, 0.020767191806299885], "location_sim": [0.0, 0.0], "pattern": -1, "time": 30}, {"A_title": " bin/elasticsearch: add help, fix endless loop", "A_clean_title": ["bin", "elasticsearch", "add", "help", "fix", "endless", "loop"], "B_title": "Startup: Prevent potential loop in bin/elasticsearch", "B_clean_title": ["startup", "prevent", "potenti", "loop", "bin", "elasticsearch"], "A_body": "This change adds command line help for all options to the es start script.\nBoth `-h` and `--help` options are accepted.\n\nAlso, an endless busy loop in the long options parser was fixed: running the script with a long opt parameter w/o value (e.g. `elasticsearch --buuuurrrnn`) the long option parser would end up in an endless busy loop.\n\nFixes #2168 \nFixes #7104\n", "A_clean_body": ["thi", "chang", "add", "command", "line", "help", "all", "option", "es", "start", "script", "both", "help", "option", "are", "accept", "also", "endless", "busi", "loop", "long", "option", "parser", "wa", "fix", "run", "script", "long", "opt", "paramet", "valu", "elasticsearch", "buuuurrrnn", "long", "option", "parser", "would", "end", "up", "endless", "busi", "loop", "fix", "2168", "fix", "7104"], "B_body": "If bin/elasticsearch is started with a single --long argument, it could\npotentially be caught up in a loop on bash (dash handles this correctly).\n\nThis commit adds an additional check that prints out an error message\n\nCloses #7104\n", "B_clean_body": ["bin", "elasticsearch", "start", "singl", "long", "argument", "it", "could", "potenti", "caught", "up", "loop", "bash", "dash", "handl", "thi", "correctli", "thi", "commit", "add", "addit", "check", "that", "print", "out", "error", "messag", "close", "7104"], "title_sim": [0.4880852970705831], "body_sim": [0.28168550873813236], "file_list_sim": 1.0, "overlap_files_len": 1, "code_sim": [0.0, 0.0], "location_sim": [0.6329113924050633, 0.6329113924050633], "pattern": 1, "time": 44}, {"A_title": "Factor out PID file creation and add tests", "A_clean_title": ["factor", "out", "pid", "file", "creation", "add", "test"], "B_title": "Bootstrap.java: fix PID file creation", "B_clean_title": ["bootstrap", "java", "fix", "pid", "file", "creation"], "A_body": "This commit factors out the PID file creation from bootstrap and adds\ntests for error conditions etc. We also can't rely on DELETE_ON_CLOSE\nsince it might not even write the file depending on the OS and JVM implementation.\nThis impl uses a shutdown hook to best-effort remove the pid file if it was written.\n\nCloses #8771\n", "A_clean_body": ["thi", "commit", "factor", "out", "pid", "file", "creation", "bootstrap", "add", "test", "error", "condit", "etc", "we", "also", "ca", "n't", "reli", "delet", "close", "sinc", "it", "might", "not", "even", "write", "file", "depend", "os", "jvm", "implement", "thi", "impl", "use", "shutdown", "hook", "best", "effort", "remov", "pid", "file", "it", "wa", "written", "close", "8771"], "B_body": "This change fixes various failure modes on PID file creation which have been\nintroduced by using nio:\n- check if PID file has a directory part\n- Allow PID directory creation fail w/ FileAlreadyExistsException\n  (thrown if directory exists and is a soft link)\n- create PID file if it doesn't exist, truncate it if it does\n\nfixes #8771\n", "B_clean_body": ["thi", "chang", "fix", "variou", "failur", "mode", "pid", "file", "creation", "which", "have", "been", "introduc", "by", "nio", "check", "pid", "file", "ha", "directori", "part", "allow", "pid", "directori", "creation", "fail", "filealreadyexistsexcept", "file", "alreadi", "exist", "except", "thrown", "directori", "exist", "soft", "link", "creat", "pid", "file", "it", "n't", "exist", "truncat", "it", "it", "fix", "8771"], "title_sim": [0.43884718106460996], "body_sim": [0.4563436125846239], "file_list_sim": 0.3333333333333333, "overlap_files_len": 1, "code_sim": [0.3382050605378865, 0.0], "location_sim": [0.12955465587044535, 0.7111111111111111], "pattern": 1, "time": 0}, {"A_title": "updating azure storage to 5.0 issue #23448", "A_clean_title": ["updat", "azur", "storag", "issu", "23448"], "B_title": "Update to Azure Storage 5.0.0", "B_clean_title": ["updat", "azur", "storag"], "A_body": "Im sorry for the pull request on the abeyad/elasticsearch, i have created a new PR on elastic/elasticsearch", "A_clean_body": ["im", "sorri", "pull", "request", "abeyad", "elasticsearch", "have", "creat", "new", "pr", "elast", "elasticsearch"], "B_body": "Closes #23448.", "B_clean_body": ["close", "23448"], "title_sim": [0.9372829399301842], "body_sim": [-0.004037777427785644], "file_list_sim": 0.16666666666666666, "overlap_files_len": 1, "code_sim": [0.0, 0.0], "location_sim": [0.8421052631578947, 1.0], "pattern": 1, "time": 0}, {"A_title": "Add infrastructure to transactionally apply and reset dynamic settings", "A_clean_title": ["add", "infrastructur", "transact", "appli", "reset", "dynam", "set"], "B_title": "add support for resetting dynamic cluster settings", "B_clean_title": ["add", "support", "reset", "dynam", "cluster", "set"], "A_body": "This commit adds the infrastructure to make settings that are updateable\nresetable and changes the application of updates to be transactional. This means\nsetting updates are either applied or not. If the application failes all values are rejected.\n\nThis initial commit converts all dynamic cluster settings to make use of the new infrastructure.\nAll cluster level dynamic settings are not resettable to their defaults or to the node level settings.\nThe infrastructure also allows to list default values and descriptions which is not fully implemented yet.\n\nValues can be reset using a list of key or simple regular expressions. This has only been implemented on the java\nlayer yet. For instance to reset all recovery settings to their defaults a user can just specify `indices.recovery.*`.\n\nThis commit also adds strict settings validation, if a setting is unknown or if a setting can not be applied the entire\nsettings update request will fail.\n", "A_clean_body": ["thi", "commit", "add", "infrastructur", "make", "set", "that", "are", "updat", "reset", "chang", "applic", "updat", "transact", "thi", "mean", "set", "updat", "are", "either", "appli", "or", "not", "applic", "fail", "all", "valu", "are", "reject", "thi", "initi", "commit", "convert", "all", "dynam", "cluster", "set", "make", "use", "new", "infrastructur", "all", "cluster", "level", "dynam", "set", "are", "not", "resett", "their", "default", "or", "node", "level", "set", "infrastructur", "also", "allow", "list", "default", "valu", "descript", "which", "not", "fulli", "implement", "yet", "valu", "reset", "list", "key", "or", "simpl", "regular", "express", "thi", "ha", "onli", "been", "implement", "java", "layer", "yet", "instanc", "reset", "all", "recoveri", "set", "their", "default", "user", "just", "specifi", "indic", "recoveri", "thi", "commit", "also", "add", "strict", "set", "valid", "set", "unknown", "or", "set", "not", "appli", "entir", "set", "updat", "request", "will", "fail"], "B_body": "This PR implements support for resetting transient & persistent cluster settings to, either the configuration file value defined at node startup, or if it does not existed, the default value.\n\nResetting settings is done using the normal cluster update settings API, by just assigning a JSON `null` to the setting one want to reset. See `update-settings.asciidoc` for details.\n\nThere are maybe other/smarter ways to do that, but I think this is just straight-forward and it works ;)\nThis is related to https://github.com/elasticsearch/elasticsearch/issues/6732, but does only apply to cluster settings not custom analyzer.\n", "B_clean_body": ["thi", "pr", "implement", "support", "reset", "transient", "persist", "cluster", "set", "either", "configur", "file", "valu", "defin", "at", "node", "startup", "or", "it", "not", "exist", "default", "valu", "reset", "set", "done", "normal", "cluster", "updat", "set", "api", "by", "just", "assign", "json", "null", "set", "one", "want", "reset", "see", "updat", "set", "asciidoc", "detail", "there", "are", "mayb", "other", "smarter", "way", "that", "but", "think", "thi", "just", "straight", "forward", "it", "work", "thi", "relat", "http", "github", "com", "elasticsearch", "elasticsearch", "issu", "6732", "but", "onli", "appli", "cluster", "set", "not", "custom", "analyz"], "title_sim": [0.6253525769001881], "body_sim": [0.6349096147860006], "file_list_sim": 0, "overlap_files_len": 0, "code_sim": [0.0, 0.0], "location_sim": [0.0, 0.0], "pattern": 0, "time": 451}, {"A_title": "Other bucket now shows if enabled on empty buckets", "A_clean_title": ["other", "bucket", "now", "show", "enabl", "empti", "bucket"], "B_title": "Setting 'other' bucket on empty aggregation", "B_clean_title": ["set", "'other", "bucket", "empti", "aggreg"], "A_body": "Previous to this commit empty buckets (with a doc count of zero) would not show the 'other' bucket in the filters aggregation. Now the buildEmptyBucket() method in FiltersAggregator checks to see if the other bucket is enabled when building an empty aggregation and adds it if it is enabled.\n\nCloses #16546\n", "A_clean_body": ["previou", "thi", "commit", "empti", "bucket", "doc", "count", "zero", "would", "not", "show", "'other", "bucket", "filter", "aggreg", "now", "buildemptybucket", "build", "empti", "bucket", "method", "filtersaggreg", "filter", "aggreg", "check", "see", "other", "bucket", "enabl", "when", "build", "empti", "aggreg", "add", "it", "it", "enabl", "close", "16546"], "B_body": "Closes #16546 \n", "B_clean_body": ["close", "16546"], "title_sim": [0.6531487411170728], "body_sim": [0.08462785357901721], "file_list_sim": 1.0, "overlap_files_len": 2, "code_sim": [0.8849262445951055, 0.8849262445951055], "location_sim": [0.17293233082706766, 0.17293233082706766], "pattern": 1, "time": 0}, {"A_title": "Remove workdir, thats unused", "A_clean_title": ["remov", "workdir", "that", "unus"], "B_title": "Remove working directory", "B_clean_title": ["remov", "work", "directori"], "A_body": "We don't need to keep around code for something we are not using.\n\nIf we ever need it, we can just revert this commit instead of maintaining dead code.\n", "A_clean_body": ["we", "n't", "need", "keep", "around", "code", "someth", "we", "are", "not", "we", "ever", "need", "it", "we", "just", "revert", "thi", "commit", "instead", "maintain", "dead", "code"], "B_body": "This commit removes the unused working directory and its associated environment variable \"WORK_DIR\"\n", "B_clean_body": ["thi", "commit", "remov", "unus", "work", "directori", "it", "associ", "environ", "variabl", "work", "dir"], "title_sim": [0.2997262897325687], "body_sim": [0.07347404683600757], "file_list_sim": 0.75, "overlap_files_len": 12, "code_sim": [0.0, 0.0], "location_sim": [0.8132911392405063, 0.9178571428571428], "pattern": 0, "time": 4}, {"A_title": "fix typo", "A_clean_title": ["fix", "typo"], "B_title": "fix typo in docs/reference/modules/cluster.asciidoc", "B_clean_title": ["fix", "typo", "asciidoc", "doc", "refer", "modul", "cluster"], "A_body": "", "A_clean_body": [], "B_body": "", "B_clean_body": [], "title_sim": [0.4599465031277356], "body_sim": [0.0], "file_list_sim": 1.0, "overlap_files_len": 1, "code_sim": [1.0, 1.0], "location_sim": [1.0, 1.0], "pattern": 0, "time": 1}, {"A_title": "Update aws sdk to 1.11.20", "A_clean_title": ["updat", "aw", "sdk", "11", "20"], "B_title": "Version bump AWS SDK 1.11.18", "B_clean_title": ["version", "bump", "aw", "sdk", "11", "18"], "A_body": "WIP: DO NOT MERGE IT (tests are failing see description)\n# AWS Release Notes\n\nFrom 1.10.69 (see #17784), here are the most important updates:\n## Minor 1.10 releases:\n- Amazon S3 Added support for a new configuration named BucketAccelerateConfiguration which supports faster uploads/downloads to S3 buckets.\n- Adding several missing throttling error codes for API Gateway and S3.\n- Amazon S3 Introducing a new version of the ListObjects (ListObjectsV2) API that allows listing objects with a large number of delete markers.\n## 1.11:\n\nAWS SDK for Java:\n- Improved URL encoding for REST clients.\n- Dropped usage of Json.org library in favor of Jackson.\n- Updated retry policies to include jitter during backoffs.\n- Generate output POJOs for all operations.\n- Renamed the aws-java-sdk-flow-build-tools-{sdkversion}.jar to aws-swf-build-tools-1.0.jar. The jar is also available in Maven.\n\nAWS SDK for Java - Amazon S3:\n- Added support to return the part count of an object in object metadata. You can also download a part by setting part number in GetObjectRequest.\n- TransferManager supports parallel downloads for multipart objects.\n- Default to Signature Version 4 signing process in all regions.\n\nApache HttpClient upgraded to 4.5.2\n# Jackson update\n\nNote that [Jackson has been updated at some point](https://github.com/aws/aws-sdk-java/blob/master/pom.xml#L115) but as we don't use dependency management anymore it has never been updated and we did not notice.\n\nSo I updated:\n- jackson-databind to 2.6.6.\n- jackson-annotations to 2.6.0.\n\nThat being said, I'm unsure if we should better use `${versions.jackson}` instead.\n# Functional changes\n\nFor `repository-s3` plugin, this new version of the AWS SDK now sets Throttle Retries (overridable by `use_throttle_retries`) to `true` by default instead of `false` previously.\n# Security manager issues\n\nTests are failing on `repository-s3` plugin with:\n\n```\n[2016-07-26 09:36:40,350][WARN ][repositories             ] [AUqzQAb] failed to create repository [s3][test_repo_s3_1]\njava.security.AccessControlException: access denied (\"java.lang.RuntimePermission\" \"accessDeclaredMembers\")\n    at java.security.AccessControlContext.checkPermission(AccessControlContext.java:472)\n    at java.security.AccessController.checkPermission(AccessController.java:884)\n    at java.lang.SecurityManager.checkPermission(SecurityManager.java:549)\n    at java.lang.Class.checkMemberAccess(Class.java:2348)\n    at java.lang.Class.getDeclaredConstructors(Class.java:2019)\n    at com.fasterxml.jackson.databind.introspect.AnnotatedClass.resolveCreators(AnnotatedClass.java:338)\n    at com.fasterxml.jackson.databind.introspect.AnnotatedClass.getStaticMethods(AnnotatedClass.java:245)\n    at com.fasterxml.jackson.databind.introspect.BasicBeanDescription.getFactoryMethods(BasicBeanDescription.java:461)\n    at com.fasterxml.jackson.databind.deser.BasicDeserializerFactory._addDeserializerFactoryMethods(BasicDeserializerFactory.java:670)\n    at com.fasterxml.jackson.databind.deser.BasicDeserializerFactory._constructDefaultValueInstantiator(BasicDeserializerFactory.java:321)\n    at com.fasterxml.jackson.databind.deser.BasicDeserializerFactory.findValueInstantiator(BasicDeserializerFactory.java:254)\n    at com.fasterxml.jackson.databind.deser.BasicDeserializerFactory.createCollectionDeserializer(BasicDeserializerFactory.java:1027)\n    at com.fasterxml.jackson.databind.deser.DeserializerCache._createDeserializer2(DeserializerCache.java:394)\n    at com.fasterxml.jackson.databind.deser.DeserializerCache._createDeserializer(DeserializerCache.java:352)\n    at com.fasterxml.jackson.databind.deser.DeserializerCache._createAndCache2(DeserializerCache.java:264)\n    at com.fasterxml.jackson.databind.deser.DeserializerCache._createAndCacheValueDeserializer(DeserializerCache.java:244)\n    at com.fasterxml.jackson.databind.deser.DeserializerCache.findValueDeserializer(DeserializerCache.java:142)\n    at com.fasterxml.jackson.databind.DeserializationContext.findContextualValueDeserializer(DeserializationContext.java:428)\n    at com.fasterxml.jackson.databind.deser.std.StdDeserializer.findDeserializer(StdDeserializer.java:947)\n    at com.fasterxml.jackson.databind.deser.BeanDeserializerBase.resolve(BeanDeserializerBase.java:439)\n    at com.fasterxml.jackson.databind.deser.DeserializerCache._createAndCache2(DeserializerCache.java:296)\n    at com.fasterxml.jackson.databind.deser.DeserializerCache._createAndCacheValueDeserializer(DeserializerCache.java:244)\n    at com.fasterxml.jackson.databind.deser.DeserializerCache.findValueDeserializer(DeserializerCache.java:142)\n    at com.fasterxml.jackson.databind.DeserializationContext.findRootValueDeserializer(DeserializationContext.java:461)\n    at com.fasterxml.jackson.databind.ObjectMapper._findRootDeserializer(ObjectMapper.java:3838)\n    at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:3732)\n    at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:2796)\n    at com.amazonaws.partitions.PartitionsLoader.loadPartitionFromStream(PartitionsLoader.java:92)\n    at com.amazonaws.partitions.PartitionsLoader.build(PartitionsLoader.java:84)\n    at com.amazonaws.regions.RegionMetadataFactory.create(RegionMetadataFactory.java:30)\n    at com.amazonaws.regions.RegionUtils.initialize(RegionUtils.java:66)\n    at com.amazonaws.regions.RegionUtils.getRegionMetadata(RegionUtils.java:54)\n    at com.amazonaws.regions.RegionUtils.getRegion(RegionUtils.java:107)\n    at com.amazonaws.services.s3.AmazonS3Client.createSigner(AmazonS3Client.java:3288)\n    at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3984)\n    at com.amazonaws.services.s3.AmazonS3Client.headBucket(AmazonS3Client.java:1218)\n    at com.amazonaws.services.s3.AmazonS3Client.doesBucketExist(AmazonS3Client.java:1175)\n    at org.elasticsearch.cloud.aws.blobstore.S3BlobStore.<init>(S3BlobStore.java:88)\n    at org.elasticsearch.repositories.s3.S3Repository.<init>(S3Repository.java:309)\n    at org.elasticsearch.plugin.repository.s3.S3RepositoryPlugin.lambda$getRepositories$6(S3RepositoryPlugin.java:73)\n    at org.elasticsearch.repositories.RepositoriesService.createRepository(RepositoriesService.java:381)\n    at org.elasticsearch.repositories.RepositoriesService.registerRepository(RepositoriesService.java:354)\n    at org.elasticsearch.repositories.RepositoriesService.access$100(RepositoriesService.java:54)\n    at org.elasticsearch.repositories.RepositoriesService$1.execute(RepositoriesService.java:107)\n    at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)\n    at org.elasticsearch.cluster.service.ClusterService.runTasksForExecutor(ClusterService.java:553)\n    at org.elasticsearch.cluster.service.ClusterService$UpdateTask.run(ClusterService.java:857)\n    at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:450)\n    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:237)\n    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:200)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\n```\n\nRelated to #18910.\n", "A_clean_body": ["wip", "not", "merg", "it", "test", "are", "fail", "see", "descript", "aw", "releas", "note", "10", "69", "see", "17784", "here", "are", "most", "import", "updat", "minor", "10", "releas", "amazon", "s3", "ad", "support", "new", "configur", "name", "bucketaccelerateconfigur", "bucket", "acceler", "configur", "which", "support", "faster", "upload", "download", "s3", "bucket", "ad", "sever", "miss", "throttl", "error", "code", "api", "gateway", "s3", "amazon", "s3", "introduc", "new", "version", "listobject", "list", "object", "listobjectsv2", "list", "object", "v2", "api", "that", "allow", "list", "object", "larg", "number", "delet", "marker", "11", "aw", "sdk", "java", "improv", "url", "encod", "rest", "client", "drop", "usag", "json", "org", "librari", "favor", "jackson", "updat", "retri", "polici", "includ", "jitter", "dure", "backoff", "gener", "output", "pojo", "poj", "os", "all", "oper", "renam", "aw", "java", "sdk", "flow", "build", "tool", "sdkversion", "jar", "aw", "swf", "build", "tool", "jar", "jar", "also", "avail", "maven", "aw", "sdk", "java", "amazon", "s3", "ad", "support", "return", "part", "count", "object", "object", "metadata", "you", "also", "download", "part", "by", "set", "part", "number", "getobjectrequest", "get", "object", "request", "transfermanag", "transfer", "manag", "support", "parallel", "download", "multipart", "object", "default", "signatur", "version", "sign", "process", "all", "region", "apach", "httpclient", "http", "client", "upgrad", "jackson", "updat", "note", "that", "jackson", "ha", "been", "updat", "at", "some", "point", "http", "sdk", "xml", "github", "com", "aw", "aw", "java", "blob", "master", "pom", "l115", "but", "as", "we", "n't", "use", "depend", "manag", "anymor", "it", "ha", "never", "been", "updat", "we", "did", "not", "notic", "so", "updat", "jackson", "databind", "jackson", "annot", "that", "be", "said", "'m", "unsur", "we", "better", "use", "version", "jackson", "instead", "function", "chang", "repositori", "s3", "plugin", "thi", "new", "version", "aw", "sdk", "now", "set", "throttl", "retri", "overrid", "by", "use", "throttl", "retri", "true", "by", "default", "instead", "fals", "previous", "secur", "manag", "issu", "test", "are", "fail", "repositori", "s3", "plugin", "2016", "07", "26", "09:36:40,350", "warn", "repositori", "auqzqab", "uqz", "ab", "fail", "creat", "repositori", "s3", "test", "repo", "s3", "java", "secur", "accesscontrolexcept", "access", "control", "except", "access", "deni", "java", "lang", "runtimepermiss", "runtim", "permiss", "accessdeclaredmemb", "access", "declar", "member", "at", "java", "secur", "accesscontrolcontext", "checkpermiss", "access", "control", "context", "check", "permiss", "accesscontrolcontext", "java:472", "access", "control", "context", "at", "java", "secur", "accesscontrol", "checkpermiss", "access", "control", "check", "permiss", "accesscontrol", "java:884", "access", "control", "at", "java", "lang", "securitymanag", "checkpermiss", "secur", "manag", "check", "permiss", "securitymanag", "java:549", "secur", "manag", "at", "java", "lang", "class", "checkmemberaccess", "check", "member", "access", "class", "java:2348", "at", "java", "lang", "class", "getdeclaredconstructor", "get", "declar", "constructor", "class", "java:2019", "at", "com", "fasterxml", "jackson", "databind", "introspect", "annotatedclass", "resolvecr", "annot", "class", "resolv", "creator", "annotatedclass", "java:338", "annot", "class", "at", "com", "fasterxml", "jackson", "databind", "introspect", "annotatedclass", "getstaticmethod", "annot", "class", "get", "static", "method", "annotatedclass", "java:245", "annot", "class", "at", "com", "fasterxml", "jackson", "databind", "introspect", "basicbeandescript", "getfactorymethod", "basic", "bean", "descript", "get", "factori", "method", "basicbeandescript", "java:461", "basic", "bean", "descript", "at", "com", "fasterxml", "jackson", "databind", "deser", "basicdeserializerfactori", "basic", "deseri", "factori", "adddeserializerfactorymethod", "add", "deseri", "factori", "method", "basicdeserializerfactori", "java:670", "basic", "deseri", "factori", "at", "com", "fasterxml", "jackson", "databind", "deser", "basicdeserializerfactori", "basic", "deseri", "factori", "constructdefaultvalueinstanti", "construct", "default", "valu", "instanti", "basicdeserializerfactori", "java:321", "basic", "deseri", "factori", "at", "com", "fasterxml", "jackson", "databind", "deser", "basicdeserializerfactori", "findvalueinstanti", "basic", "deseri", "factori", "find", "valu", "instanti", "basicdeserializerfactori", "java:254", "basic", "deseri", "factori", "at", "com", "fasterxml", "jackson", "databind", "deser", "basicdeserializerfactori", "createcollectiondeseri", "basic", "deseri", "factori", "creat", "collect", "deseri", "basicdeserializerfactori", "java:1027", "basic", "deseri", "factori", "at", "com", "fasterxml", "jackson", "databind", "deser", "deserializercach", "deseri", "cach", "createdeserializer2", "creat", "deserializer2", "deserializercach", "java:394", "deseri", "cach", "at", "com", "fasterxml", "jackson", "databind", "deser", "deserializercach", "deseri", "cach", "createdeseri", "creat", "deseri", "deserializercach", "java:352", "deseri", "cach", "at", "com", "fasterxml", "jackson", "databind", "deser", "deserializercach", "deseri", "cach", "createandcache2", "creat", "cache2", "deserializercach", "java:264", "deseri", "cach", "at", "com", "fasterxml", "jackson", "databind", "deser", "deserializercach", "deseri", "cach", "createandcachevaluedeseri", "creat", "cach", "valu", "deseri", "deserializercach", "java:244", "deseri", "cach", "at", "com", "fasterxml", "jackson", "databind", "deser", "deserializercach", "findvaluedeseri", "deseri", "cach", "find", "valu", "deseri", "deserializercach", "java:142", "deseri", "cach", "at", "com", "fasterxml", "jackson", "databind", "deserializationcontext", "findcontextualvaluedeseri", "deseri", "context", "find", "contextu", "valu", "deseri", "deserializationcontext", "java:428", "deseri", "context", "at", "com", "fasterxml", "jackson", "databind", "deser", "std", "stddeseri", "finddeseri", "std", "deseri", "find", "deseri", "stddeseri", "java:947", "std", "deseri", "at", "com", "fasterxml", "jackson", "databind", "deser", "beandeserializerbas", "resolv", "bean", "deseri", "base", "beandeserializerbas", "java:439", "bean", "deseri", "base", "at", "com", "fasterxml", "jackson", "databind", "deser", "deserializercach", "deseri", "cach", "createandcache2", "creat", "cache2", "deserializercach", "java:296", "deseri", "cach", "at", "com", "fasterxml", "jackson", "databind", "deser", "deserializercach", "deseri", "cach", "createandcachevaluedeseri", "creat", "cach", "valu", "deseri", "deserializercach", "java:244", "deseri", "cach", "at", "com", "fasterxml", "jackson", "databind", "deser", "deserializercach", "findvaluedeseri", "deseri", "cach", "find", "valu", "deseri", "deserializercach", "java:142", "deseri", "cach", "at", "com", "fasterxml", "jackson", "databind", "deserializationcontext", "findrootvaluedeseri", "deseri", "context", "find", "root", "valu", "deseri", "deserializationcontext", "java:461", "deseri", "context", "at", "com", "fasterxml", "jackson", "databind", "objectmapp", "object", "mapper", "findrootdeseri", "find", "root", "deseri", "objectmapp", "java:3838", "object", "mapper", "at", "com", "fasterxml", "jackson", "databind", "objectmapp", "object", "mapper", "readmapandclos", "read", "map", "close", "objectmapp", "java:3732", "object", "mapper", "at", "com", "fasterxml", "jackson", "databind", "objectmapp", "readvalu", "object", "mapper", "read", "valu", "objectmapp", "java:2796", "object", "mapper", "at", "com", "amazonaw", "partit", "partitionsload", "loadpartitionfromstream", "partit", "loader", "load", "partit", "stream", "partitionsload", "java:92", "partit", "loader", "at", "com", "amazonaw", "partit", "partitionsload", "build", "partit", "loader", "partitionsload", "java:84", "partit", "loader", "at", "com", "amazonaw", "region", "regionmetadatafactori", "creat", "region", "metadata", "factori", "regionmetadatafactori", "java:30", "region", "metadata", "factori", "at", "com", "amazonaw", "region", "regionutil", "initi", "region", "util", "regionutil", "java:66", "region", "util", "at", "com", "amazonaw", "region", "regionutil", "getregionmetadata", "region", "util", "get", "region", "metadata", "regionutil", "java:54", "region", "util", "at", "com", "amazonaw", "region", "regionutil", "getregion", "region", "util", "get", "region", "regionutil", "java:107", "region", "util", "at", "com", "amazonaw", "servic", "s3", "amazons3cli", "createsign", "amazon", "s3client", "creat", "signer", "amazons3cli", "java:3288", "amazon", "s3client", "at", "com", "amazonaw", "servic", "s3", "amazons3cli", "invok", "amazon", "s3client", "amazons3cli", "java:3984", "amazon", "s3client", "at", "com", "amazonaw", "servic", "s3", "amazons3cli", "headbucket", "amazon", "s3client", "head", "bucket", "amazons3cli", "java:1218", "amazon", "s3client", "at", "com", "amazonaw", "servic", "s3", "amazons3cli", "doesbucketexist", "amazon", "s3client", "bucket", "exist", "amazons3cli", "java:1175", "amazon", "s3client", "at", "org", "elasticsearch", "cloud", "aw", "blobstor", "s3blobstor", "s3blob", "store", "init", "s3blobstor", "java:88", "s3blob", "store", "at", "org", "elasticsearch", "repositori", "s3", "s3repositori", "init", "s3repositori", "java:309", "at", "org", "elasticsearch", "plugin", "repositori", "s3", "s3repositoryplugin", "lambda", "s3repositori", "plugin", "getrepositori", "get", "repositori", "s3repositoryplugin", "java:73", "s3repositori", "plugin", "at", "org", "elasticsearch", "repositori", "repositoriesservic", "createrepositori", "repositori", "servic", "creat", "repositori", "repositoriesservic", "java:381", "repositori", "servic", "at", "org", "elasticsearch", "repositori", "repositoriesservic", "registerrepositori", "repositori", "servic", "regist", "repositori", "repositoriesservic", "java:354", "repositori", "servic", "at", "org", "elasticsearch", "repositori", "repositoriesservic", "access", "repositori", "servic", "100", "repositoriesservic", "java:54", "repositori", "servic", "at", "org", "elasticsearch", "repositori", "repositoriesservic", "repositori", "servic", "execut", "repositoriesservic", "java:107", "repositori", "servic", "at", "org", "elasticsearch", "cluster", "clusterstateupdatetask", "execut", "cluster", "state", "updat", "task", "clusterstateupdatetask", "java:45", "cluster", "state", "updat", "task", "at", "org", "elasticsearch", "cluster", "servic", "clusterservic", "runtasksforexecutor", "cluster", "servic", "run", "task", "executor", "clusterservic", "java:553", "cluster", "servic", "at", "org", "elasticsearch", "cluster", "servic", "clusterservic", "cluster", "servic", "updatetask", "run", "updat", "task", "clusterservic", "java:857", "cluster", "servic", "at", "org", "elasticsearch", "common", "util", "concurr", "threadcontext", "thread", "context", "contextpreservingrunn", "run", "context", "preserv", "runnabl", "threadcontext", "java:450", "thread", "context", "at", "org", "elasticsearch", "common", "util", "concurr", "prioritizedesthreadpoolexecutor", "priorit", "es", "thread", "pool", "executor", "tiebreakingprioritizedrunn", "runandclean", "tie", "break", "priorit", "runnabl", "run", "clean", "prioritizedesthreadpoolexecutor", "java:237", "priorit", "es", "thread", "pool", "executor", "at", "org", "elasticsearch", "common", "util", "concurr", "prioritizedesthreadpoolexecutor", "priorit", "es", "thread", "pool", "executor", "tiebreakingprioritizedrunn", "run", "tie", "break", "priorit", "runnabl", "prioritizedesthreadpoolexecutor", "java:200", "priorit", "es", "thread", "pool", "executor", "at", "java", "util", "concurr", "threadpoolexecutor", "runwork", "thread", "pool", "executor", "run", "worker", "threadpoolexecutor", "java:1142", "thread", "pool", "executor", "at", "java", "util", "concurr", "threadpoolexecutor", "thread", "pool", "executor", "worker", "run", "threadpoolexecutor", "java:617", "thread", "pool", "executor", "at", "java", "lang", "thread", "run", "thread", "java:745", "relat", "18910"], "B_body": "- bump version\n- fixed issue with double md5 signature generation\n", "B_clean_body": ["bump", "version", "fix", "issu", "doubl", "md5", "signatur", "gener"], "title_sim": [0.67595965621507], "body_sim": [0.02181489250738867], "file_list_sim": 0.28125, "overlap_files_len": 9, "code_sim": [0.3898961467233794, 0.992762906489548], "location_sim": [0.18930390492359933, 0.6317280453257791], "pattern": -1, "time": 7}, {"A_title": "Add ingest info to node info API, which contains a list of available processors", "A_clean_title": ["add", "ingest", "info", "node", "info", "api", "which", "contain", "list", "avail", "processor"], "B_title": "Add Ingest info to api and validate processors exist across cluster", "B_clean_title": ["add", "ingest", "info", "api", "valid", "processor", "exist", "across", "cluster"], "A_body": "Internally the put pipeline API uses this information in node info API to validate if all specified processors in a pipeline exist on all nodes in the cluster.\n", "A_clean_body": ["intern", "put", "pipelin", "api", "use", "thi", "inform", "node", "info", "api", "valid", "all", "specifi", "processor", "pipelin", "exist", "all", "node", "cluster"], "B_body": "- Adds IngestInfo to _nodes api\n- Adds /_ingest/processors rest endpoint to fetch processors across\n  cluster\n- Adds getProcessors to client\n- Validates that all processors defined in a stored or simulated\n  pipeline exist on all ingest nodes across the cluster\n", "B_clean_body": ["add", "ingestinfo", "ingest", "info", "node", "api", "add", "ingest", "processor", "rest", "endpoint", "fetch", "processor", "across", "cluster", "add", "getprocessor", "get", "processor", "client", "valid", "that", "all", "processor", "defin", "store", "or", "simul", "pipelin", "exist", "all", "ingest", "node", "across", "cluster"], "title_sim": [0.6258802816839948], "body_sim": [0.5783750167497848], "file_list_sim": 0.5, "overlap_files_len": 14, "code_sim": [0.9043256124662425, 0.9882347959097673], "location_sim": [0.5935828877005348, 0.9507494646680942], "pattern": 0, "time": 24}, {"A_title": "Internal: Remove Strings.cleanPath", "A_clean_title": ["intern", "remov", "string", "cleanpath", "clean", "path"], "B_title": "add utest for Strings cleanPath", "B_clean_title": ["add", "utest", "string", "cleanpath", "clean", "path"], "A_body": "This commit removes the cleanPath method, in favor of using java's\r\nPath.normalize().", "A_clean_body": ["thi", "commit", "remov", "cleanpath", "clean", "path", "method", "favor", "java'", "path", "normal"], "B_body": "When add a unit test for method \"cleanPath\" of Strings in elasticsearch/common package, the test results turn out to be far from its function description:\nNormalize the path by suppressing sequences like \"\"path/..\"and inner simple dots\n--that means path like \"/root/../home\" would be suppressed as path \"/home\", but actually the test case failed.\n--the below example original paths would be suppressed as the Expected normalized path, but actually not:\n **Original**                                **Expetced:**                  **Actual:**  \n\"/root//home\"                             \"/root/home\"                 \"/root//home\"\n\"/root/../home\"                           \"/home\"                         \"/root/../home\"\n\"/root/../home\"                           \"home\"                          \"/root/../home\"\n\"file:./root/../home/elk\"              \"file:home/elk\"               \"file:./root/../home/elk\"\nSome common util classes and methods deserve more attention : )\n\nCloses #20851 \n", "B_clean_body": ["when", "add", "unit", "test", "method", "cleanpath", "clean", "path", "string", "elasticsearch", "common", "packag", "test", "result", "turn", "out", "far", "it", "function", "descript", "normal", "path", "by", "suppress", "sequenc", "like", "path", "inner", "simpl", "dot", "that", "mean", "path", "like", "root", "home", "would", "suppress", "as", "path", "home", "but", "actual", "test", "case", "fail", "below", "exampl", "origin", "path", "would", "suppress", "as", "expect", "normal", "path", "but", "actual", "not", "**original**", "**expetc", "**actual", "root", "home", "root", "home", "root", "home", "root", "home", "home", "root", "home", "root", "home", "home", "root", "home", "file", "root", "home", "elk", "file", "home", "elk", "file", "root", "home", "elk", "some", "common", "util", "class", "method", "deserv", "more", "attent", "close", "20851"], "title_sim": [0.8340332041970243], "body_sim": [0.3037591063113963], "file_list_sim": 0.25, "overlap_files_len": 1, "code_sim": [0.0, 0.0], "location_sim": [0.0, 0.0], "pattern": 0, "time": 245}, {"A_title": "Add option to set path style access for AWS S3 client. Usefull for so\u2026", "A_clean_title": ["add", "option", "set", "path", "style", "access", "aw", "s3", "client", "useful", "so\u2026"], "B_title": "Add support for path_style_access", "B_clean_title": ["add", "support", "path", "style", "access"], "A_body": "Add option to set path style access for AWS S3 client. Needed for some ceph installationme ceph installation\n", "A_clean_body": ["add", "option", "set", "path", "style", "access", "aw", "s3", "client", "need", "some", "ceph", "installationm", "ceph", "instal"], "B_body": "From https://github.com/elastic/elasticsearch-cloud-aws/pull/159\n\nAdd a new option `path_style_access` for S3 buckets. It adds support for path style access for [virtual hosting of buckets](http://docs.aws.amazon.com/AmazonS3/latest/dev/VirtualHosting.html).\nDefaults to `false`.\n\nCloses https://github.com/elastic/elasticsearch-cloud-aws/issues/124.\n", "B_clean_body": ["http", "cloud", "aw", "pull", "159", "github", "com", "elast", "elasticsearch", "add", "new", "option", "path", "style", "access", "s3", "bucket", "it", "add", "support", "path", "style", "access", "virtual", "host", "bucket", "http", "aw", "amazon", "html", "doc", "com", "amazons3", "latest", "dev", "virtualhost", "amazon", "s3", "virtual", "host", "default", "fals", "close", "http", "cloud", "aw", "issu", "124", "github", "com", "elast", "elasticsearch"], "title_sim": [0.5008013930536327], "body_sim": [0.5666048162801919], "file_list_sim": 0.0, "overlap_files_len": 0, "code_sim": [0.7835295235944957, 0.0], "location_sim": [0.0, 0.0], "pattern": 0, "time": 147}, {"A_title": "Ingest: Add script processor", "A_clean_title": ["ingest", "add", "script", "processor"], "B_title": "new ScriptProcessor for Ingest", "B_clean_title": ["new", "scriptprocessor", "script", "processor", "ingest"], "A_body": "This adds another processor, that supports scripting for fields.\nRight now it works by using the return value of a script to update\na specific field, as this blends better into the ingest processor infrastructure\nthan updating a full document.\n\nAnother possibility would be to have the same as the Update API, where you\ncan set arbitrary fields in the map and then store this map as the new source.\n\nCurrent example call:\n\n```\ncurl -X PUT localhost:9200/_ingest/pipeline/script-pipeline -d '\n{\n  \"description\" : \"script pipeline\",\n  \"processors\" : [\n    {\n    \"script\" : {\n      \"field\" : \"bytes_total\",\n      \"lang\" : \"painless\",\n      \"inline\" : \"return input.ctx._source.doc.bytes_in + input.ctx._source.doc.bytes_out\"\n    }\n  }\n ]\n}\n'\n\ncurl -X POST localhost:9200/_ingest/pipeline/script-pipeline/_simulate -d '\n{\n  \"docs\" : [\n    { \"_source\" : { \"bytes_in\" : 1024, \"bytes_out\" : 2048 } }\n  ]\n}\n'\n```\n\nTO DISCUSS: Alternatively behave like the Update API and allow to configure anything. However this is much\nharder to implement, as the IngestDocument only allows for single field updates and a complete replacement\nof the ingest document would be more tricky and resource intensive.\n\n```\ncurl -X PUT localhost:9200/_ingest/pipeline/script-pipeline -d '\n{\n  \"description\" : \"script pipeline\",\n  \"processors\" : [\n    {\n      \"script\" : {\n        \"lang\" : \"painless\",\n        \"inline\" : \"input.ctx._source.bytes_total = input.ctx._source.doc.bytes_in + input.ctx._source.doc.bytes_out\"\n      }\n    }\n  ]\n}\n'\n```\n", "A_clean_body": ["thi", "add", "anoth", "processor", "that", "support", "script", "field", "right", "now", "it", "work", "by", "return", "valu", "script", "updat", "specif", "field", "as", "thi", "blend", "better", "into", "ingest", "processor", "infrastructur", "than", "updat", "full", "document", "anoth", "possibl", "would", "have", "same", "as", "updat", "api", "where", "you", "set", "arbitrari", "field", "map", "then", "store", "thi", "map", "as", "new", "sourc", "current", "exampl", "call", "curl", "put", "pipelin", "localhost:9200", "ingest", "pipelin", "script", "descript", "script", "pipelin", "processor", "script", "field", "byte", "total", "lang", "painless", "inlin", "return", "input", "ctx", "doc", "sourc", "byte", "input", "ctx", "doc", "sourc", "byte", "out", "curl", "post", "simul", "localhost:9200", "ingest", "pipelin", "script", "pipelin", "doc", "sourc", "byte", "1024", "byte", "out", "2048", "discuss", "altern", "behav", "like", "updat", "api", "allow", "configur", "anyth", "howev", "thi", "much", "harder", "implement", "as", "ingestdocu", "ingest", "document", "onli", "allow", "singl", "field", "updat", "complet", "replac", "ingest", "document", "would", "more", "tricki", "resourc", "intens", "curl", "put", "pipelin", "localhost:9200", "ingest", "pipelin", "script", "descript", "script", "pipelin", "processor", "script", "lang", "painless", "inlin", "input", "ctx", "sourc", "byte", "total", "input", "ctx", "doc", "sourc", "byte", "input", "ctx", "doc", "sourc", "byte", "out"], "B_body": "Enable running arbitrary scripts from within an ingest pipeline.\n", "B_clean_body": ["enabl", "run", "arbitrari", "script", "within", "ingest", "pipelin"], "title_sim": [0.9131420277628599], "body_sim": [0.5339749504359387], "file_list_sim": 0.0, "overlap_files_len": 0, "code_sim": [0.9305759235133002, 0.0], "location_sim": [0.0, 0.0], "pattern": 0, "time": 2}, {"A_title": "Create $PID_DIR if not exists", "A_clean_title": ["creat", "pid", "dir", "not", "exist"], "B_title": "Create PID_DIR in init.d script", "B_clean_title": ["creat", "pid", "dir", "init", "script"], "A_body": "When you istall elasticsearch with the deb package (apt-get install elasticsearch), you have an error when starting elasticsearch :\nsudo service elasticsearch start\n[....] Starting Elasticsearch Server:touch: cannot touch `/var/run/elasticsearch                                                                    /elasticsearch.pid': No such file or directory\nfailed!\n\nIf the $PID_DIR doesn't exist, elasticsearch can't start.\n", "A_clean_body": ["when", "you", "istal", "elasticsearch", "deb", "packag", "apt", "get", "instal", "elasticsearch", "you", "have", "error", "when", "start", "elasticsearch", "sudo", "servic", "elasticsearch", "start", "start", "elasticsearch", "server", "touch", "not", "touch", "var", "run", "elasticsearch", "pid", "elasticsearch", "no", "such", "file", "or", "directori", "fail", "pid", "dir", "n't", "exist", "elasticsearch", "ca", "n't", "start"], "B_body": "Since the /var/run/elasticsearch directory is cleaned when the operating system starts, the init.d script must ensure that the PID_DIR is correctly created.\n\nCloses #11594\n", "B_clean_body": ["sinc", "var", "run", "elasticsearch", "directori", "clean", "when", "oper", "system", "start", "init", "script", "must", "ensur", "that", "pid", "dir", "correctli", "creat", "close", "11594"], "title_sim": [0.480624708734676], "body_sim": [0.34075595902770645], "file_list_sim": 0.25, "overlap_files_len": 1, "code_sim": [0.0, 0.0], "location_sim": [0.0, 0.0], "pattern": 0, "time": 20}]